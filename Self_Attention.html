<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <title>Self Attention</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
        }
        h1 {
            text-align: center;
        }
        .article-content {
            max-width: 1000px;
            margin: 0 auto;
        }
        .image {
            text-align: center;
        }
		.justified {
            text-align: justify;
	    }
		
    </style>
</head>
<body>
    <h2><center>Self Attention</center></h2>
    <div class="article-content">
        <p class="justified">
		Most of the popular language models are Transformer-based architectures that use an important 
		technique called 'self-attention'. The 'self-attention' is a little different from the attention
		mechanism used in the RNN-based encoder-decoder model. Let’s first try to understand the intuition
		before delving into mathematical details and equations.
		<br><br>

		The primary function of self-attention is to generate context-aware vectors from the input sequence 
		itself rather than considering both input and output as in the RNN-based encoder-decoder architecture.
		See the example below shown in Fig. 1 (Ref. Deep Learning with Python by François Chollet).  
		</p>
        
        <div class="image">
            <img src="https://www.dropbox.com/scl/fi/m2q6nu8isizp6sfy5p7rs/attention_matrix.png?rlkey=gi7xk6hx7q0wrtje5px5zvexy&raw=1" alt="Photo 1" width="800" height="550">
        
            <p>Figure 1</p>
        </div>
       
        <p class="justified">
		In this example, there are 7 sequences in the sentence 'the train left the station on time', and we can see 
		there is a 7x7 attention score matrix. For the time being, let's say we somehow obtained these attention score values. 
		<br><br>
		According to the self-attention scores depicted in the picture, the word 'train' pays more attention to the word 
		'station' rather than other words in consideration, such as 'on' or 'the'. Alternatively, we can say the word 
		'station' pays more attention to the word 'train' rather than other words in consideration, such as 'on' or 'the'. 
		<br><br>
		The attention scores of each word in a sequence with all other words can be calculated, and the same is 
		shown in the figure as a 7x7 score matrix. The self-attention model allows inputs to interact with each
		other (i.e., calculate the attention of all other inputs with one input).
		<br><br>
		Attention scores help in understanding the contextual meaning of a word in a given sentence. For example, here the
		word 'station' has been used in the context of a train station, not in other contexts like a gas station or a bus 
		station, etc.
		<br><br>
		The attention score is computed through cosine similarity, i.e., the dot product of two-word vectors, which assesses
		the strength of their relationship or the degree of similarity between the words being compared. Obviously, there 
		are many other mathematical aspects to consider, which will be discussed subsequently.
		<br><br>
		These attention scores are utilized as weights for calculating the weighted sum of all the words in the sentence. 
		For example, when representing the word 'station', the words closely related to 'station' will contribute more to 
		the sum (including the word 'station' itself), while irrelevant words will contribute almost nothing. The resulting 
		vector serves as a new representation for 'station': one that incorporates the surrounding context. Specifically, 
		it includes part of the 'train' vector, thereby clarifying that it is, indeed, a 'train station'.
		<br><br>
		This process is iterated for every word in the sentence, yielding a new sequence of vectors that encode the sentence.
		
		</p>
		
		<h3><center>Calculation of Attention weights</center></h3>
		
		<p class="justified">
		Before delving into the mathematical details of the self-attention mechanism, let's revisit the equations and terms 
		derived from the attention mechanism in RNN-based encoder-decoder architectures (as explained in my initial article:
		'Birth of Attention Mechanism'). Although it's not necessary, to begin with the RNN-based attention mechanism to grasp
		self-attention, comparing term by term would facilitate comprehension and help us understand how identical concepts 
		are used in both. 
		<br><br> 
		A conceptual diagram is shown in Fig. 2 below to depict the attention mechanism in the RNN-based encoder-decoder model,
		where symbols have their usual meaning.		
		</p>
		
		 <div class="image">
            <img src="https://www.dropbox.com/scl/fi/abffpsrv93ipa04xz91y8/context_vector.png?rlkey=8bw6fftxl0kekjjcys7n5id2e&raw=1" alt="Photo 2" width="400" height="300">
            <p>Figure 2</p>
        </div>
	
		
		<p class="justified">
		<b>Recall, in case of Attention in RNN based encoder-decoder model:</b>
		<br>
		<b>Context vector: </b>\( C(i)=\sum_{j=1}^{T_x} \alpha_{i,j}h(j) \)
		<br>
		<b>score: \( e_{i,j} = f( S_{i-1}, h_j ) \)</b>
		<br>
		<b>Attention Score (SoftMax of score): \( \alpha_{i,j} = \frac{ \exp(e_{i,j})}{\sum_{k=1}^{T} \exp(e_{i,k})}  \) </b>
		<br><br>
		
		In Self-Attention, we get rid of RNN units and calculate the context-aware vectors from the input sequence itself. 
		A conceptual diagram is shown in Fig. 3 below to depict the Self-attention. Suppose we have a sequence of feature 
		vectors (embeddings) \( X_1\), \(X_2\) all the way up to \(X_T\), in this case, T equals 4. We can use the concept of cosine similarity,
		which measures sameness or relatedness, to calculate scores and finally obtain the SoftMax, which can be used in the
		context vector as given below.
		<br><br>
		In case of Self attention, score is cosine similarity, i.e. dot product of input sequence 
		with itself.<br><br>
		Thus, <br>
		<b>score: \( X_i ^{T}. X_j \)</b>; [Dot product of input vector with itself.]
		<br>
		<b>Attention Score: \( \alpha_{i,j} = SoftMax(score/const.)\) </b> [Dividing by a constant to control the variance]
		<br>
		<b>Self-Attention Vector: </b>\( A(i)=\sum_{j=1}^{T_x} \alpha_{i,j}x(j) \)
		<br><br>
		
		Here, h(j) is replaced by  \( x_j \) and notation is changed from C to A as we call it ‘Attention Vector’ instead of ‘context
		vector’.
		
	
		
		<div class="image">
            <img src="https://www.dropbox.com/scl/fi/ymanzzg4dt8931bzxbuul/attention_vector.png?rlkey=p0faapuk400xh4lwms38e73nx&raw=1" alt="Photo 3" width="350" height="300">
            <p>Figure 3</p>
        </div>
		
		<b>Self-Attention in vectorized form: </b>\( SoftMax( \frac{X_i ^{T}. X_j } {const.}). x_j \)
		<br><br>
		Table 1 shown below presents the side-by-side comparison of Self Attention Mechanism and RNN based Attention Mechanism.
		<br><br>
		<h4><center>Table 1:  Comparison of Self attention and RNN based Attention</center></h4>
		
		<div class="image">
            <img src="https://www.dropbox.com/scl/fi/c27q03cqx6fhtbw494lvs/table_comparison.png?rlkey=7xxdrefzkyxe82sxxzq744gjw&raw=1" alt="Photo 3" width="700" height="300">
        </div>
				
		</p>
		
		<h3><center>Introducing Queries(Q), Keys(K), and Values(V)</center></h3>
		
		
		
		<p class="justified">
		We computed the Self-Attention vector, as derived above, based on the inputs of the vectors themselves.
		This means that for fixed inputs, these attention weights would always be fixed. In other words, there are no 
		learnable parameters. This is problematic and needs to be fixed by introducing some learnable parametersthat
		will make the self-attention mechanism more flexible and tunable for various tasks. To fulfill this purpose,
		three trainable weight matrices are introduced and multiplied with input \( X_i \) separately, and three new terms
		Queries(Q), Keys(K), and Values(V) come into the picture as given by the equations below. Vectorized implementation
		& Shape tracking are also shown in subsequent steps.
		<br><br>
		Assume input ‘X’ is a sequence of ‘T’ time steps or in simple words a sentence with ‘T’ words and each word is 
		represented by an Embedding vector of dimension  '\( d_{model} \)'. Fig. 4 below is showing multiple sentence samples 
		(Xi’s) in rows S1, S2, S3, and so on, and words by column as ti=1, 2, 3, ..., ti=T, which is say, up to 100.
		
		<div class="image">
            <img src="https://www.dropbox.com/scl/fi/2321cxr164o094me7zwxr/input_data_sample.png?rlkey=c1rf6deq7u0vdk7hr9gdyfl04&raw=1" alt="Photo 3" width="500" height="275">
            <p>Figure 4</p>
        </div>
		
		Notation and dimensions are followed as per the paper <a href="https://arxiv.org/pdf/1706.03762v6.pdf">
			‘Attention Is All You Need’</a>. Table 2 of shapes and Fig. 5 are taken from this paper.
		
		<br><br>
		shape of \( X \rightarrow T \times d_{model} \)<br><br>
		\( d_{model} \)= Embedding vector for each word (512 as per the paper)<br><br>
		Queries: \( Q = XW^Q \);  Where \(W^Q\) is weight matrix introduced to calculate Q from X.<br><br>
		Keys: \( K = XW^K \);  Where \(W^K\) is weight matrix introduced to calculate K from X.<br><br>
		Valuess: \( V = XW^V \);  Where \(W^V\) is weight matrix introduced to calculate V from X.<br><br>
		
		<b>Shape tracking \( \rightarrow \) (shape of X ) Matrix multiplication (shape of Weight Matrix ) \( \rightarrow  \)Output shape</b><br><br>
		\(  Q = XW^Q \rightarrow (T \times d_{model})* (d_{model} × d_k) \rightarrow (T \times d_k) \) <br><br>
		\(  K = XW^K \rightarrow (T \times d_{model})* (d_{model} × d_k) \rightarrow (T \times d_k) \)  <br><br>
		\(  V = XW^V \rightarrow (T \times d_{model})* (d_{model} × d_v) \rightarrow (T \times d_v) \) <br><br>
		
		Finally, substituting  Q, K, V, and const. \( = \sqrt{d_k}  \) ( \( d_k \)  dimensionality of the key vector) in above 
		self-attention eqn. (I), we get equation for attention as given below:<br><br>
		
		<center><b>Attention(Q,K,V) = \( SoftMax( \frac{QK^{T}} {\sqrt{d_k}})V \)</b>     ---------(II) </center>
		<br><br>
		Fig.5 below is a graphical representation of obtaining Q, K, and V from input X and feeding into equation (II) above.<br><br>
		
		<div class="image">
            <img src="https://www.dropbox.com/scl/fi/mmx2ekdcfvwahtnig251k/scaled_dor_product_depiction1.png?rlkey=eizuqemm5svuqvpv4ggb9q22i&raw=1" alt="Photo 3" width="750" height="300">
            <p>Figure 5 <a href="https://velog.io/@jiyoung/Transformer-구현하고-이해하기2">(Ref.)</a><p>
        </div>
		
		Above attention eqn. (II) is also termed as Scaled Dot Product Attention depicted by Fig. 6 given below.
		</p>
		
		<p align="left">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
		&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Scaled Dot-Product Attention &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
		&nbsp; &nbsp; Table 2. Shapes <br>
		
		<div class="image">
            <img src="https://www.dropbox.com/scl/fi/d469scgn818g6uty0aw3b/scaled_dot_product_shapes.png?rlkey=pkbbtuc5t65b407a84dq5e2p5&raw=1" alt="Photo 3" width="600" height="280">
            <p align="left"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
			&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Figure 6 </p>
        </div>
		
		Shape of \(  QK^T \rightarrow (T \times d_k)* (d_k × T) \rightarrow (T \times T) \) <br><br>
		And finally, the shape of final attention output: \( (T \times T) * (T  \times d_v) → (T \times d_v)  \)<br><br>
		If we consider a batch of N samples at a time for processing, then above shape will be: \( N \times T \times d_v \).
		<br><br>
		Please note that the shapes of \( W^Q, W^K, W^V \)are chosen in a way that matrix multiplication with input
		‘X’ is possible. The values of  \( d_k, d_v , d_{model} \) are hyperparameters and the table of shapes above 
		shows the values used by the author in the paper.
		
		</p>
				 
		<h3> <center>Database Analogy for Queries(Q), Keys(K) and Values(V)</center></h3>
		<p class="justified">
		
		In the context of databases, queries are used to interact with databases to retrieve or manipulate data,
		keys are used to uniquely identify records and establish relationships between tables, and values are the 
		actual data stored in the fields of a database table. The same kind of analogy exists in Self-attention 
		Q, K, and V as well, as shown in Fig. 7 below.
		
		<div class="image">
            <img src="https://www.dropbox.com/scl/fi/w30v8iv0lz7iimpuu4mqm/database_inspiration.png?rlkey=3qsrob2h5vvu2kjoxt0e49hmo&raw=1" alt="Photo 3" width="550" height="200">
            <p>Figure 7<p>
        </div>
		In this example, the word 'check' acts as a query, and all the words in the sequence act as keys. 
		Attention weights find the answer to the question: 'Which key or what are the keys that match with 
		the query?' by assigning different weights to the keys. Here, the word 'check' is paying more attention
		to the words'cashed' and 'bank' in the sentence, as represented by the thickness of the joining lines.
		<br><br>
		In self-attention, every word acts as a query once, while the entire words in the sequence act as keys, 
		and attention weights are calculated to figure out which key matches with the query. 
		<br><br>
		Finally, the words (represented as vectors) are treated as values, and attention weights are used to form 
		a weighted sum of the values, resulting in an attention vector.The attention vector for the word 'check'
		will be the weighted sum of the value vectors.
		
		</p>
		
		
		<h3> <center>How does self-attention help in contextual understanding?</center></h3>
		
		<p class="justified">
		In the example we looked at, we were figuring out the meaning of a word. So, if we just see the word 'check' 
		by itself, it could mean different things. But when we look at the other words in the sentence, like 'cashed' 
		and 'bank', it helps us understand that in this context 'check' refers to a financial document,
		not something like checking your homework or check in chess.
		<br><br>
		<b>A few points to notice:</b>
		<ul>
		<li> Every word must have an attention weight with every other word, i.e., for T number of terms, T2 
			computations are required to calculate attention weights.</li>

		<li> Q, K, and V are calculated independently, resulting in parallelization, unlike in RNN where
		h(t-1) must be computed before h(t). </li>

		<li> Self-attention can handle sequences of any length, unlike RNN, which has trouble with vanishing 
		gradients.</li>

		</ul>
		 
		</p>
     
    </div>
</body>
</html>
