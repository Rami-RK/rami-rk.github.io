<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Transformer Model Architecture | Ramendra Kumar </title> <meta name="author" content="Ramendra Kumar"> <meta name="description" content="From encoders to masked attention and decoders - Transformer architecture demystified."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rami-rk.github.io/blog/2024/transformer-architecture/"> <script src="/assets/js/theme.js?v=48c9b5bd7f2e0605e39e579400e22553"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Transformer Model Architecture",
            "description": "From encoders to masked attention and decoders - Transformer architecture demystified.",
            "published": "March 27, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Ramendra</span> Kumar </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Transformer Model Architecture</h1> <p>From encoders to masked attention and decoders - Transformer architecture demystified.</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#overview">Overview</a> </div> <div> <a href="#encoder">Encoder</a> </div> <div> <a href="#multi-head-attention">Multi-Head Attention</a> </div> <div> <a href="#positional-encoding">Positional Encoding</a> </div> <div> <a href="#causal-or-masked-attention-in-decoder-block">Causal or Masked Attention in Decoder Block</a> </div> <div> <a href="#why-does-multi-head-attention-work">Why Does Multi-Head Attention Work</a> </div> </nav> </d-contents> <h2 id="overview">Overview</h2> <p>Figure 1 shows the Transformer architecture from the “Attention Is All You Need” paper (2017). The left block is the <strong>Encoder</strong> and the right block is the <strong>Decoder</strong>. This encoder-decoder architecture mirrors RNN-based seq2seq in its input-output flow, but removes recurrent components. The decoder still behaves as a causal model.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/tfr_full_archi-480.webp 480w,/assets/img/transformer/tfr_full_archi-800.webp 800w,/assets/img/transformer/tfr_full_archi-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/transformer/tfr_full_archi.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 1</figcaption> </figure> <p>The encoder sends values (V) and queries (Q) to the decoder in a cross-attention connection, allowing the decoder to focus on relevant parts of the encoded input when producing the output sequence.</p> <p>Encoders and decoders can also stand alone. BERT is encoder-only, GPT is decoder-only, and T5 combines both.</p> <p>Both blocks include Positional Encoding, Multi-Head Attention, Add &amp; Norm (skip connection + layer norm), and Feed Forward layers. The decoder uses causal (masked) attention so each token attends only to previous tokens.</p> <p><strong>Mastering the Encoder block makes the entire Transformer easier to understand.</strong></p> <hr> <h2 id="encoder">Encoder</h2> <p>The encoder consists of Transformer blocks as shown in Fig. 2 (simplified and expanded in the right panel).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/enc_unfolded-480.webp 480w,/assets/img/transformer/enc_unfolded-800.webp 800w,/assets/img/transformer/enc_unfolded-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/transformer/enc_unfolded.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 2</figcaption> </figure> <p>The Transformer block can be sub-divided into two main sub-layers:</p> <ul> <li>A <strong>multi-head attention</strong> sub-layer that receives queries, keys, and values.</li> <li>A <strong>feed-forward network</strong> sub-layer.</li> </ul> <p>Each sub-layer is followed by layer normalization and a residual connection. Dropout is applied before layer norm for regularization (not shown in the diagram).</p> <p>The feed-forward network is typically two linear layers with a ReLU activation in between. The encoder is formed by stacking these blocks multiple times (shown as N in Fig. 2).</p> <p>BERT (Bi-directional Encoder Representations) is an encoder-only architecture that stacks multiple transformer blocks and a prediction head, as shown in Fig. 3.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/tfr_blocks_assembed-480.webp 480w,/assets/img/transformer/tfr_blocks_assembed-800.webp 800w,/assets/img/transformer/tfr_blocks_assembed-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/transformer/tfr_blocks_assembed.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 3</figcaption> </figure> <hr> <h2 id="multi-head-attention">Multi-Head Attention</h2> <p>Multi-Head Attention (MHA) runs multiple attention heads in parallel. Each head splits Q, K, and V independently, computes attention, and the results are concatenated and projected to produce the final output. This helps the model capture multiple relationships in the sequence.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/mha-480.webp 480w,/assets/img/transformer/mha-800.webp 800w,/assets/img/transformer/mha-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/transformer/mha.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 4</figcaption> </figure> <p>Shape tracking (Fig. 5):</p> <ul> <li>Input shape to each attention layer: \((T \times d_{model})\)</li> <li>Output shape after each head: \((T \times d_V)\)</li> <li>Output after concatenation: \((T \times h d_V)\)</li> </ul> <p>Final projection:</p> \[\text{Output} = \text{Concat}(A_1, A_2, \dots, A_h) W^O\] <ul> <li> \[\text{Concat}(A_1, \dots, A_h) \rightarrow (T \times h d_V)\] </li> <li> \[W^O \rightarrow (h d_V \times d_{model})\] </li> <li>Output shape: \((T \times d_{model})\)</li> </ul> <blockquote> <p><strong>Back to the initial input shape.</strong></p> </blockquote> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/mh_concat-480.webp 480w,/assets/img/transformer/mh_concat-800.webp 800w,/assets/img/transformer/mh_concat-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/transformer/mh_concat.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 5</figcaption> </figure> <p>Each head operates independently, so attention calculations are parallel.</p> <hr> <h2 id="positional-encoding">Positional Encoding</h2> <p>Without RNNs, attention is permutation invariant, so token order is lost. Positional encoding adds order information to embeddings by injecting a unique pattern of values at each position.</p> <p>Two common approaches:</p> <ol> <li>Trainable positional embeddings (sub-class the embedding layer).</li> <li>Fixed sinusoidal encoding (as in the original paper).</li> </ol> <p>Paper formulas:</p> \[PE_{(pos,\,2i)}=\sin\!\left(\frac{pos}{10000^{2i/d_{model}}}\right)\] \[PE_{(pos,\,2i+1)}=\cos\!\left(\frac{pos}{10000^{2i/d_{model}}}\right)\] <hr> <h2 id="causal-or-masked-attention-in-decoder-block">Causal or Masked Attention in Decoder Block</h2> <p>Decoder blocks are causal. When predicting a token, the decoder should only attend to previous tokens, not future tokens.</p> <p>For a sequence length \(T\), the attention score matrix is \(T \times T\). We enforce causality by masking the upper triangle so that attention weights are non-zero only when \(i \ge j\).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/masking-480.webp 480w,/assets/img/transformer/masking-800.webp 800w,/assets/img/transformer/masking-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/transformer/masking.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 6</figcaption> </figure> <p>This masked attention is used inside decoder blocks. GPT-style models are decoder-only architectures built on this mechanism.</p> <p>The decoder with masked MHA is shown in Fig. 7:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/decoder_with_masked_mha-480.webp 480w,/assets/img/transformer/decoder_with_masked_mha-800.webp 800w,/assets/img/transformer/decoder_with_masked_mha-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/transformer/decoder_with_masked_mha.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 7</figcaption> </figure> <p>The full encoder-decoder connection is shown in Fig. 8:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/enc_dec_connection-480.webp 480w,/assets/img/transformer/enc_dec_connection-800.webp 800w,/assets/img/transformer/enc_dec_connection-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/transformer/enc_dec_connection.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 8</figcaption> </figure> <hr> <h2 id="why-does-multi-head-attention-work">Why Does Multi-Head Attention Work</h2> <p>Multi-head attention lets the model learn multiple types of relationships in parallel. For example, one head might capture <em>where</em> a subject goes, while another captures <em>what</em> the subject did. This allows richer representations of sequences.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/intuition_mha-480.webp 480w,/assets/img/transformer/intuition_mha-800.webp 800w,/assets/img/transformer/intuition_mha-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/transformer/intuition_mha.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 9</figcaption> </figure> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/" target="_blank" rel="external nofollow noopener">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2" target="_blank" rel="external nofollow noopener">Displaying External Posts on Your al-folio Blog</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/self-attention/">Self Attention</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/photo-gallery/">a post with image galleries</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/tabs/">a post with tabs</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Ramendra Kumar. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=6f508d74becd347268a7f822bca7309d"></script> </body> </html>