<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Transformer Model Architecture | Ramendra Kumar </title> <meta name="author" content="Ramendra Kumar"> <meta name="description" content="From encoders to masked attention and decoders - Transformer architecture demystified."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rami-rk.github.io/blog/2024/transformer-architecture/"> <script src="/assets/js/theme.js?v=48c9b5bd7f2e0605e39e579400e22553"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Transformer Model Architecture",
            "description": "From encoders to masked attention and decoders - Transformer architecture demystified.",
            "published": "March 27, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Ramendra</span> Kumar </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About Me </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search (Ctrl+K)" onclick="openSearchModal()"> <span class="nav-link"><i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Transformer Model Architecture</h1> <p>From encoders to masked attention and decoders - Transformer architecture demystified.</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#overview">Overview</a> </div> <div> <a href="#encoder">Encoder</a> </div> <div> <a href="#multi-head-attention">Multi-Head Attention</a> </div> <div> <a href="#positional-encoding">Positional Encoding</a> </div> <div> <a href="#causal-or-masked-attention-in-decoder-block">Causal or Masked Attention in Decoder Block</a> </div> <div> <a href="#why-does-multi-head-attention-work">Why Does Multi-Head Attention Work</a> </div> </nav> </d-contents> <h3 id="overview">Overview</h3> <p>In my attempt to demystify the Transformer Architecture, this is the third and last article. Here, I have explained all the components and their details in a very simplified way. Please read my previous two articles: <a href="/blog/2024/birth-of-attention-mechanism/">1. Birth of Attention Mechanism</a>, and <a href="/blog/2025/self-attention/">2. Self-Attention</a>, to fully appreciate and comprehend this article. These three articles will assist you in clarifying your understanding of the transformer, which serves as the foundation for recent advancements such as BERT, GPT, ChatGPT, and LLMs. Please note that these articles are not superficial overviews but rather technical in nature.</p> <p>The Fig.1 given below shows a Transformer Model Architecture as per the paper <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="external nofollow noopener">‘Attention Is All You Need’</a>. published in 2017 by A. Vaswani et al. It has two blocks — the left block is the Encoder and the right block is the Decoder. This transformer-based encoder-decoder architecture operates similarly to the RNN-based encoder-decoder that we saw by a translation example in the previous article — <a href="/blog/2024/birth-of-attention-mechanism/">Birth of Attention Mechanism</a> but without the presence of an RNN component. Despite this difference, the input-output feeding mechanism to the encoder-decoder remains consistent. Additionally, akin to the RNN model, the decoder functions as a causal model.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/tfr_full_archi-480.webp 480w,/assets/img/transformer/tfr_full_archi-800.webp 800w,/assets/img/transformer/tfr_full_archi-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/transformer/tfr_full_archi.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 1</figcaption> </figure> <p>The encoder sends Values(V) and Queries (Q) to the decoder, linking them together to create the entire Transformer system. This connection, known as cross-attention, enables the decoder to pay attention to the encoded information from the input sequence provided by the encoder. With this mechanism, the decoder can concentrate on different sections of the input sequence when producing the output sequence.</p> <p>It’s important to understand that the encoder and decoder components can function independently as standalone models for various tasks. For instance, BERT serves as an exclusive encoder architecture, while GPT operates solely as a decoder architecture. On the other hand, T5 incorporates both encoder and decoder components within its architecture.</p> <p>The Encoder and Decoder blocks share similar components, including Positional Encoding, Multi-Head Attention, Add &amp; Norm (comprising Skip connection and Layer normalization), and Feed Forward. However, one thing to notice is that as the decoder functions as a causal model, it employs causal attention or masked attention. This means that each token is restricted to attending only to its preceding tokens, not the subsequent ones.</p> <p><strong>Mastering the Encoder block facilitates comprehension of the Decoder block and the entire Transformer architecture. Let’s see Encoder in detail.</strong></p> <hr> <h3 id="encoder">Encoder</h3> <p>The Encoder consists of a transformer block as shown in Fig. 2. A simplified and expanded diagram is also shown on the right side. Let’s understand the internal components of a Transformer block.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/enc_unfolded-480.webp 480w,/assets/img/transformer/enc_unfolded-800.webp 800w,/assets/img/transformer/enc_unfolded-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/transformer/enc_unfolded.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 2</figcaption> </figure> <p>The <strong>Transformer block</strong> can be sub-divided into <strong>two main sub-layers</strong>:</p> <ul> <li>The first sub-layer comprises a <strong>multi-head attention mechanism</strong> that receives the queries, keys, and values as inputs.</li> <li>A second sub-layer comprises a fully connected <strong>feed-forward network</strong>.</li> </ul> <p>Following each of these two sub-layers is layer normalization, into which the sub-layer input (through a residual/skip connection) and output are fed. Regularization is also introduced into the model by applying a dropout to the output of each sub-layer (before the layer normalization step) which is not shown in the figure.</p> <p>The feed-forward network allows the model to extract higher-level features from the input. This network usually comprises two linear layers with a ReLU activation function in between. The feed-forward network allows the model to extract deeper meaning from the input data and more compactly and usefully represent the input. In the paper, an ANN with one hidden layer and a ReLu activation in the middle with no activation function at the output layer has been implemented.</p> <p>The encoder is formed by repeated transformer block joined one after another multiple times which is shown as multiplied by N in Fig.2 above. The transformer encoder is a crucial part of the transformer encoder-decoder architecture, which is widely used for natural language processing tasks.</p> <p><strong>BERT</strong> which stands for Bi-directional encoder representation is an encoder-only architecture that consists of 6 to 12 such transformer blocks and a prediction head as the last layer, shown in Fig.3 below.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/tfr_blocks_assembed-480.webp 480w,/assets/img/transformer/tfr_blocks_assembed-800.webp 800w,/assets/img/transformer/tfr_blocks_assembed-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/transformer/tfr_blocks_assembed.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 3</figcaption> </figure> <hr> <h3 id="multi-head-attention">Multi-Head Attention</h3> <p>Please go through the previous article on the <a href="/blog/2025/self-attention/">Self-Attention</a> mechanism, which is implemented in Multi-Head Attention. In Multi-Head Attention, the Attention module repeats its computations multiple times in parallel. Each of these is called an Attention Head. The Attention module splits its Query, Key, and Value parameters N-ways and passes each split independently through a separate Head. All of these similar Attention calculations are then combined to produce a final Attention score. This is called multi-head attention and gives the Transformer greater power to encode multiple relationships and nuances for each word. A schematic representation of MHA is shown in Fig.4 below.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/mha-480.webp 480w,/assets/img/transformer/mha-800.webp 800w,/assets/img/transformer/mha-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/transformer/mha.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 4</figcaption> </figure> <p>Shape tracking and related calculations are shown with the help of Fig. 5.:</p> <ul> <li>Input shape to each attention layer : \((T \times d_{model})\)</li> <li>Output shape after each attention layer : \((T \times d_V)\)</li> <li>After concatenation, the shape of Output (concatenate them along the feature dimension) : \((T \times h d_V)\)</li> </ul> <p>Imagine that the output of each self-attention block lined up side by side after concatenation.</p> <p><strong>Final projection:</strong> \(\text{Output} = \text{Concat}(A_1, A_2, \dots, A_h) W^O\)</p> <ul> <li>Shape of : \(\text{Concat}(A_1, \dots, A_h) \rightarrow (T \times h d_V)\)</li> <li>Shape of : \(W^O \rightarrow (h d_V \times d_{model})\)</li> <li>Shape of final Output : \(\text{Concat}(A_1, A_2, \dots, A_h) W^O \rightarrow (T \times h d_V) \times (h d_V \times d_{model})\)</li> </ul> <p>\(\rightarrow (T \times d_{model})\) &gt; <strong>Back to the initial input shape.</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/mh_concat-480.webp 480w,/assets/img/transformer/mh_concat-800.webp 800w,/assets/img/transformer/mh_concat-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/transformer/mh_concat.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 5</figcaption> </figure> <p>We can notice in the above figure that each attention calculation is functioning parallelly and has no dependency on the other.</p> <hr> <h3 id="positional-encoding">Positional Encoding</h3> <p>Passing embeddings directly into the transformer block results in missing information about the order of tokens as we get rid of RNN blocks and attention is permutation invariant i.e. order of the token does not matter to attention. Although transformers are a sequence model, it appears that this important detail has somehow been lost. Positional encoding is for rescue. Positional encoding adds positional information to the existing embeddings.</p> <p>A unique set of numbers is added at each position of the existing embeddings, such that this new set of numbers can uniquely identify which position they are located at. The following two ways are there to add positional encoding:</p> <ol> <li> <p>Positional Encoding by Sub-Classing the Embedding Layer (Trainable)</p> </li> <li> <p>Positional Encoding scheme as per the paper (non-trainable)</p> </li> </ol> <p>In the scheme suggested in the paper, the encoding is created by using a set of sins and cosines at different frequencies. The paper uses the following formula for calculating the positional encoding.</p> \[PE_{(pos,\,2i)}=\sin\!\left(\frac{pos}{10000^{2i/d_{model}}}\right)\] \[PE_{(pos,\,2i+1)}=\cos\!\left(\frac{pos}{10000^{2i/d_{model}}}\right)\] <hr> <h3 id="causal-or-masked-attention-in-decoder-block">Causal or Masked Attention in Decoder Block</h3> <p>All the above-mentioned components are found in both the encoder and decoder blocks. However, the decoder block utilizes a Causal Attention/Masked Attention Mechanism, differing slightly from the Attention mechanism in the encoder block. Let’s explore further.</p> <p>Decoder is a causal model i.e. it acts like a text generation tool, similar to how forecasts are made in time series analysis, where it predicts the next element based on previous ones. To accomplish this, when generating output at any position in the sequence, the model should only be permitted to consider the preceding tokens, excluding any tokens that followed.</p> <p>Let’s see mathematically how to achieve this through an attention score matrix. Fig.6(a) below shows the attention score matrix for just 5 tokens for the sake of demonstration. It will have a shape of \(T \times T\) for \(T\) sequences. Here, the <strong>attention score</strong> : \(\alpha (i, j)\) represents how much \(i\) pays attention to \(j\).</p> <p>Is it possible to restrict attention weights to consider only past input tokens? Yes, through some manipulation, we can achieve this: \(\alpha (i, j) &gt; 0\) only when \(i \ge j\), resulting in the score matrix depicted in Fig.6(b).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/masking-480.webp 480w,/assets/img/transformer/masking-800.webp 800w,/assets/img/transformer/masking-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/transformer/masking.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 6</figcaption> </figure> <p>We have masked the upper triangular elements to set them to zero. Thus, rather than employing plain Multi-Head Attention, we utilize a masked version known as ‘Causal Self-Attention.</p> <p>Finally, the <strong>Decoder</strong> is depicted in Fig. 7 below, which is equivalent to the Encoder, but the transformer block implements Masked Multi-Head Attention. All <strong>GPTs(Generative Pre-trained Transformers)</strong> are versions of decoder-only architecture with many more components attached to them, trained on a humongous amount of data.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/decoder_with_masked_mha-480.webp 480w,/assets/img/transformer/decoder_with_masked_mha-800.webp 800w,/assets/img/transformer/decoder_with_masked_mha-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/transformer/decoder_with_masked_mha.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 7</figcaption> </figure> <p>We have seen both the Encoder and Decoder separately. Encoder-only architecture is BERT while Decoder-only Architecture is GPT. Similarly, the architecture that contains both Encoder and Decoder is called <strong>T5 i.e. `Text to Text Transfer Transformer’</strong>. Fig. 8 below shows how Encoder and Decoder are connected in full Encoder and Decoder architecture.</p> <div style="text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/enc_dec_connection-480.webp 480w,/assets/img/transformer/enc_dec_connection-800.webp 800w,/assets/img/transformer/enc_dec_connection-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/transformer/enc_dec_connection.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 50%; " data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 8</figcaption> </figure> </div> <hr> <h3 id="why-does-multi-head-attention-work">Why Does Multi-Head Attention Work</h3> <p>It allows the layer to learn multiple features. Think of attention outputs as features that tell us something about the sentence. For example, see Fig. 9 below, when Suman pays attention to bank, the feature might be Where did Suman go? But we might also be interested in what Suman did. He cashed the check. We can only do this if we allow our layer to produce multiple features, each of which has the Suman token pay attention to different tokens in the sentence. There would be multiple and different kind of relationships and dependencies between different tokens in any sequence. Multi-head attention helps in learning those different relationships by learning corresponding multiple features.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/intuition_mha-480.webp 480w,/assets/img/transformer/intuition_mha-800.webp 800w,/assets/img/transformer/intuition_mha-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/transformer/intuition_mha.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 9</figcaption> </figure> <div class="support-section"> <h3>Support My Work</h3> <p> If you found this article helpful, consider buying me a coffee! Your support helps me create more content. </p> <a href="https://ramenkarna.gumroad.com/coffee" target="_blank" rel="noopener" class="support-button"> <i class="fa-solid fa-mug-hot"></i> Buy Me a Coffee </a> </div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4" style="margin-top: 1.5rem;">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/continuous-distribution/">Continuous Distribution &amp; PDF</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/normal-distribution/">Normal Distribution</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/random-variables/">Random Variables</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/understanding-derivatives/">Understanding Derivatives</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/self-attention/">Self Attention</a> </li> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Ramendra Kumar. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-0CKQ8QPL30"></script> <script defer src="/assets/js/google-analytics-setup.js"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=6f508d74becd347268a7f822bca7309d"></script> </body> </html>