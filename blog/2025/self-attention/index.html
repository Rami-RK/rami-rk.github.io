<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Self Attention | Ramendra Kumar </title> <meta name="author" content="Ramendra Kumar"> <meta name="description" content="From intuition to scaled dot-product attention."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rami-rk.github.io/blog/2025/self-attention/"> <script src="/assets/js/theme.js?v=48c9b5bd7f2e0605e39e579400e22553"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Self Attention",
            "description": "From intuition to scaled dot-product attention.",
            "published": "March 26, 2025",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Ramendra</span> Kumar </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About Me </a> </li> <li class="nav-item active"> <a class="nav-link" href="/">About Me <span class="sr-only">(current)</span> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Self Attention</h1> <p>From intuition to scaled dot-product attention.</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#overview">Overview</a> </div> <div> <a href="#calculation-of-attention-weights">Calculation of Attention Weights</a> </div> <div> <a href="#introducing-queries-keys-and-values">Introducing Queries, Keys, and Values</a> </div> <div> <a href="#database-analogy-for-queries-keys-and-values">Database Analogy for Queries, Keys, and Values</a> </div> <div> <a href="#how-self-attention-helps-in-contextual-understanding">How Self-Attention Helps in Contextual Understanding</a> </div> </nav> </d-contents> <h2 id="overview">Overview</h2> <p>Most popular language models are Transformer-based and use a technique called self-attention. Self-attention differs from the attention mechanism used in RNN-based encoder-decoder models. This section builds intuition before the math.</p> <p>The primary function of self-attention is to generate context-aware vectors from the input sequence itself, rather than considering both input and output as in the RNN encoder-decoder architecture.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/self-attention/attention_matrix-480.webp 480w,/assets/img/self-attention/attention_matrix-800.webp 800w,/assets/img/self-attention/attention_matrix-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/self-attention/attention_matrix.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 1</figcaption> </figure> <p>In this example, there are 7 words in the sentence “the train left the station on time” and a 7x7 attention score matrix. The word “train” pays more attention to “station” than to other words like “on” or “the.”</p> <p>Attention scores help in understanding the contextual meaning of a word in a sentence. Here “station” is used in the context of a train station, not a gas station or bus station.</p> <p>The attention score is computed through cosine similarity (dot product), which assesses the relationship between words.</p> <p>These scores are used as weights to compute a weighted sum of all words in the sentence. When representing the word “station,” the words closely related to “station” contribute more to the sum, producing a context-aware vector.</p> <blockquote> <p><strong>Note</strong> The attention vector and weights can be written as:</p> \[A(i) = \sum_{j=1}^{T_x} \alpha(i,j) x_j\] </blockquote> <p>Unfolding the equation for the word “train”:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/self-attention/unfolding_attention_vector-480.webp 480w,/assets/img/self-attention/unfolding_attention_vector-800.webp 800w,/assets/img/self-attention/unfolding_attention_vector-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/self-attention/unfolding_attention_vector.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 1 (unfolding)</figcaption> </figure> <p>This process is repeated for every word in the sentence, yielding a new sequence of context-aware vectors.</p> <hr> <h2 id="calculation-of-attention-weights">Calculation of Attention Weights</h2> <p>Before diving into self-attention, it helps to recall how attention is computed in RNN-based encoder-decoder models.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/self-attention/context_vector-480.webp 480w,/assets/img/self-attention/context_vector-800.webp 800w,/assets/img/self-attention/context_vector-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/self-attention/context_vector.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 2</figcaption> </figure> <blockquote> <p><strong>Recall: RNN Attention</strong></p> <p><strong>Context vector:</strong> \(C(i) = \sum_{j=1}^{T_x} \alpha_{i,j} h(j)\)</p> <p><strong>Score:</strong> \(e_{i,j} = f(S_{i-1}, h_j)\)</p> <p><strong>Attention score:</strong> \(\alpha_{i,j} = \frac{\exp(e_{i,j})}{\sum_{k=1}^{T} \exp(e_{i,k})}\)</p> </blockquote> <p>In self-attention, we remove RNN units and compute context-aware vectors from the input sequence itself. Suppose we have a sequence of feature vectors (embeddings) \(X_1, X_2, \dots, X_T\). We use dot products to compute scores and apply SoftMax.</p> <blockquote> <p><strong>Self-Attention</strong></p> <p><strong>Score:</strong> \(X_i^T X_j\)</p> <p><strong>Attention score:</strong> \(\alpha_{i,j} = \text{SoftMax}(\text{score} / \text{const.})\)</p> <p><strong>Attention vector:</strong> \(A(i) = \sum_{j=1}^{T_x} \alpha_{i,j} x(j)\)</p> </blockquote> <p>Here \(h(j)\) is replaced by \(x_j\) and we call the output the “attention vector.”</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/self-attention/attention_vector-480.webp 480w,/assets/img/self-attention/attention_vector-800.webp 800w,/assets/img/self-attention/attention_vector-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/self-attention/attention_vector.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 3</figcaption> </figure> <p>Self-attention in vectorized form:</p> \[\text{SoftMax}\left(\frac{X_i^T X_j}{\text{const.}}\right) \cdot x_j \quad \text{(I)}\] <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/self-attention/table_comparison-480.webp 480w,/assets/img/self-attention/table_comparison-800.webp 800w,/assets/img/self-attention/table_comparison-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/self-attention/table_comparison.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Table 1: Comparison of self-attention and RNN-based attention</figcaption> </figure> <hr> <h2 id="introducing-queries-keys-and-values">Introducing Queries, Keys, and Values</h2> <p>The formulation above has no learnable parameters. To make self-attention trainable, we introduce three weight matrices and compute Queries (Q), Keys (K), and Values (V):</p> <blockquote> <p><strong>Definitions</strong></p> <p><strong>Queries:</strong> \(Q = XW^Q\)</p> <p><strong>Keys:</strong> \(K = XW^K\)</p> <p><strong>Values:</strong> \(V = XW^V\)</p> </blockquote> <p>Assume the input is a sequence with length \(T\) and embedding dimension \(d_{model}\).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/self-attention/input_data_sample-480.webp 480w,/assets/img/self-attention/input_data_sample-800.webp 800w,/assets/img/self-attention/input_data_sample-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/self-attention/input_data_sample.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 4</figcaption> </figure> <p>Shape tracking:</p> <p>\(Q = XW^Q \rightarrow (T \times d_{model}) \cdot (d_{model} \times d_k) \rightarrow (T \times d_k)\) \(K = XW^K \rightarrow (T \times d_{model}) \cdot (d_{model} \times d_k) \rightarrow (T \times d_k)\) \(V = XW^V \rightarrow (T \times d_{model}) \cdot (d_{model} \times d_v) \rightarrow (T \times d_v)\)</p> <p>Substituting into (I), with \(\text{const.} = \sqrt{d_k}\), we get:</p> \[\text{Attention}(Q, K, V) = \text{SoftMax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V \quad \text{(II)}\] <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/self-attention/scaled_dor_product_depiction1-480.webp 480w,/assets/img/self-attention/scaled_dor_product_depiction1-800.webp 800w,/assets/img/self-attention/scaled_dor_product_depiction1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/self-attention/scaled_dor_product_depiction1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 5</figcaption> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/self-attention/scaled_dot_product_shapes1_m-480.webp 480w,/assets/img/self-attention/scaled_dot_product_shapes1_m-800.webp 800w,/assets/img/self-attention/scaled_dot_product_shapes1_m-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/self-attention/scaled_dot_product_shapes1_m.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 6</figcaption> </figure> <p>Shape of \(QK^T\):</p> \[(T \times d_k) \times (d_k \times T) \rightarrow (T \times T)\] <p>Final output shape:</p> \[(T \times T) \times (T \times d_v) \rightarrow (T \times d_v)\] <p>For batch size \(N\), the shape becomes \(N \times T \times d_v\). The values of \(d_k, d_v, d_{model}\) are hyperparameters chosen to make the matrix multiplications valid.</p> <hr> <h2 id="database-analogy-for-queries-keys-and-values">Database Analogy for Queries, Keys, and Values</h2> <p>In databases, queries retrieve data, keys identify records, and values contain the data. A similar analogy applies to self-attention Q, K, and V:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/self-attention/database_inspiration-480.webp 480w,/assets/img/self-attention/database_inspiration-800.webp 800w,/assets/img/self-attention/database_inspiration-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/self-attention/database_inspiration.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 7</figcaption> </figure> <p>In self-attention, every word acts as a query once, while all words in the sequence act as keys. Attention weights determine which keys match a query. Values are then combined using these weights to form the attention vector.</p> <hr> <h2 id="how-self-attention-helps-in-contextual-understanding">How Self-Attention Helps in Contextual Understanding</h2> <p>If we see the word “check” alone, it could mean different things. When we look at surrounding words like “cashed” and “bank,” we understand the context is a financial document rather than a chess move or a homework check.</p> <blockquote> <p><strong>Notes</strong></p> <ul> <li>Each word attends to every other word. For \(T\) words, there are \(T^2\) attention computations.</li> <li>Q, K, and V are computed in parallel (unlike RNNs which are sequential).</li> <li>Self-attention handles long sequences better than RNNs and avoids vanishing gradients.</li> </ul> </blockquote> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/" target="_blank" rel="external nofollow noopener">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2" target="_blank" rel="external nofollow noopener">Displaying External Posts on Your al-folio Blog</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/photo-gallery/">a post with image galleries</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/tabs/">a post with tabs</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/typograms/">a post with typograms</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Ramendra Kumar. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=6f508d74becd347268a7f822bca7309d"></script> </body> </html>