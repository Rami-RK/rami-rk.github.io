<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://rami-rk.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://rami-rk.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-12T18:56:50+00:00</updated><id>https://rami-rk.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Self Attention</title><link href="https://rami-rk.github.io/blog/2025/self-attention/" rel="alternate" type="text/html" title="Self Attention"/><published>2025-03-26T00:00:00+00:00</published><updated>2025-03-26T00:00:00+00:00</updated><id>https://rami-rk.github.io/blog/2025/self-attention</id><content type="html" xml:base="https://rami-rk.github.io/blog/2025/self-attention/"><![CDATA[<h3 id="overview">Overview</h3> <p>Most of the popular language models are Transformer-based architectures that use an important technique called ‘self-attention’. The ‘self-attention’ is a little different from the attention mechanism used in the RNN-based encoder-decoder model. Let’s first try to understand the intuition before delving into mathematical details and equations.</p> <p>The primary function of self-attention is to generate context-aware vectors from the input sequence itself rather than considering both input and output as in the RNN-based encoder-decoder architecture. See the example below shown in Fig. 1 (Ref. Deep Learning with Python by François Chollet).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/self-attention/attention_matrix-480.webp 480w,/assets/img/self-attention/attention_matrix-800.webp 800w,/assets/img/self-attention/attention_matrix-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/self-attention/attention_matrix.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 1</figcaption> </figure> <p>In this example, there are 7 sequences in the sentence ‘the train left the station on time’, and we can see a 7x7 attention score matrix. For the time being, let’s say we somehow obtained these attention score values.</p> <p>According to the self-attention scores depicted in the picture, the word ‘train’ pays more attention to the word ‘station’ rather than other words in consideration, such as ‘on’ or ‘the’. Alternatively, we can say the word ‘station’ pays more attention to the word ‘train’ rather than other words in consideration, such as ‘on’ or ‘the’.</p> <p>The attention scores of each word in a sequence with all other words can be calculated, and the same is shown in the figure as a 7x7 score matrix. The self-attention model allows inputs to interact with each other (i.e., calculate the attention of all other inputs with one input).</p> <p>Attention scores help in understanding the contextual meaning of a word in a given sentence. For example, here the word ‘station’ has been used in the context of a train station, not in other contexts like a gas station or a bus station, etc.</p> <p>The attention score is computed through cosine similarity, i.e., the dot product of two-word vectors, which assesses the strength of their relationship or the degree of similarity between the compared words. There are many other mathematical aspects to consider, which will be discussed subsequently.</p> <p>These attention scores are utilized as weights for calculating the weighted sum of all the words in the sentence. For example, when representing the word ‘station’, the words closely related to ‘station’ will contribute more to the sum (including the word ‘station’ itself), while irrelevant words will contribute almost nothing. The resulting vector serves as a new representation of the word ‘station’, incorporating the surrounding context. Specifically, it includes part of the ‘train’ vector, thereby clarifying that it is, indeed, a ‘train station’.</p> <p>Let’s understand the above paragraph by writing a mathematical equation as given below, where \(A(i), \alpha(i,j)\) are weighted sum (termed as attention vector) and attention scores (weights) respectively, and individual word vector is represented by \(x_j\) . \(T_x\) represents the total number of terms in the sequence.</p> <p><strong>Note</strong> : The attention vector and weights can be written as:</p> \[A(i) = \sum_{j=1}^{T_x} \alpha(i,j) x_j\] <p>Unfolding the equation for the word “train”:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/self-attention/unfolding_attention_vector-480.webp 480w,/assets/img/self-attention/unfolding_attention_vector-800.webp 800w,/assets/img/self-attention/unfolding_attention_vector-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/self-attention/unfolding_attention_vector.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 1 (unfolding)</figcaption> </figure> <p>This process is iterated for every word in the sentence, yielding a new sequence of vectors that encode the sentence.</p> <hr/> <h3 id="calculation-of-attention-weights">Calculation of Attention Weights</h3> <p>Before delving into the mathematical details of the self-attention mechanism, let’s revisit the equations and terms derived from the attention mechanism in RNN-based encoder-decoder architectures (as explained in my initial article: <a href="/blog/2024/birth-of-attention-mechanism/">Birth of Attention Mechanism</a> ). Although it’s not necessary, to begin with the RNN-based attention mechanism to grasp self-attention, comparing term by term would facilitate comprehension and help us understand how identical concepts are used in both.</p> <p>A conceptual diagram is shown in Fig. 2 below to depict the attention mechanism in the RNN-based encoder-decoder model, where symbols have their usual meaning.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/self-attention/context_vector-480.webp 480w,/assets/img/self-attention/context_vector-800.webp 800w,/assets/img/self-attention/context_vector-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/self-attention/context_vector.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 65%; " data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 2</figcaption> </figure> <p><strong>Recall: in the case of Attention in RNN based encoder-decoder model:</strong></p> <p><strong>Context vector:</strong> \(C(i) = \sum_{j=1}^{T_x} \alpha_{i,j} h(j)\)</p> <p><strong>Score:</strong> \(e_{i,j} = f(S_{i-1}, h_j)\)</p> <p><strong>Attention score</strong> (SoftMax of score): \(\alpha_{i,j} = \frac{\exp(e_{i,j})}{\sum_{k=1}^{T} \exp(e_{i,k})}\)</p> <h3 id="self-attention">Self-Attention</h3> <p>In Self-Attention, we get rid of RNN units and calculate the context-aware vectors from the input sequence itself. A conceptual diagram is shown in Fig. 3 below to depict the Self-attention. Suppose we have a sequence of feature vectors (embeddings) \(X_1, X_2, \dots, X_T\)., in this case, T equals 4. We can use the concept of cosine similarity, which measures sameness or relatedness, to calculate scores and finally obtain the SoftMax, which can be used in the context vector as given below.</p> <p>In the case of Self-attention, the score is cosine similarity, i.e. dot product of the input sequence with itself. Thus,</p> <p><strong>Score:</strong> \(X_i^T X_j\) [Dot product of input vector with itself.]</p> <p><strong>Attention score:</strong> \(\alpha_{i,j} = \text{SoftMax}(\text{score} / \text{const.})\) [Dividing by a constant to control the variance.]</p> <p><strong>Self-Attention vector:</strong> \(A(i) = \sum_{j=1}^{T_x} \alpha_{i,j} x(j)\)</p> <p>Here \(h(j)\) is replaced by \(x_j\) and notation is changed from <strong>C</strong> to <strong>A</strong> as we call it <strong>“Attention Vector”</strong> insted of “context vector.”</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/self-attention/attention_vector-480.webp 480w,/assets/img/self-attention/attention_vector-800.webp 800w,/assets/img/self-attention/attention_vector-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/self-attention/attention_vector.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 65%; " data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 3</figcaption> </figure> <p><strong>Self-attention in vectorized form:</strong></p> \[\text{SoftMax}\left(\frac{X_i^T X_j}{\text{const.}}\right) \cdot x_j \quad ------ \text{(I)}\] <p>Table 1 shown below presents the side-by-side comparison of Self Attention Mechanism and RNN based Attention Mechanism.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/self-attention/table_comparison-480.webp 480w,/assets/img/self-attention/table_comparison-800.webp 800w,/assets/img/self-attention/table_comparison-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/self-attention/table_comparison.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Table 1: Comparison of self-attention and RNN-based attention</figcaption> </figure> <hr/> <h3 id="introducing-queries-keys-and-values">Introducing Queries, Keys, and Values</h3> <p>We computed the Self-Attention vector, as derived above, based on the inputs of the vectors themselves. This means that for fixed inputs, these attention weights would always be fixed. In other words, there are no learnable parameters. This is problematic and needs to be fixed by introducing some learnable parameters that will make the self-attention mechanism more flexible and tunable for various tasks. To fulfill this purpose, three trainable weight matrices are introduced and multiplied with input Xi separately, and three new terms Queries(Q), Keys(K), and Values(V) come into the picture as given by the equations below. Vectorized implementation &amp; Shape tracking are also shown in subsequent steps.</p> <p>Assume input ‘X’ is a sequence of ‘T’ time steps or in simple words a sentence with ‘T’ words and each word is represented by an Embedding vector of dimension ‘d_model’. Fig. 4 below shows multiple sentence samples (Xi’s) in rows S1, S2, S3, and so on, and words by column as ti=1, 2, 3, …, ti=T, which is say, up to 100.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/self-attention/input_data_sample-480.webp 480w,/assets/img/self-attention/input_data_sample-800.webp 800w,/assets/img/self-attention/input_data_sample-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/self-attention/input_data_sample.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 4</figcaption> </figure> <p>Notation and dimensions are followed as per the paper <a href="https://arxiv.org/abs/1706.03762" target="_blank">Attention Is All You Need</a>. Table 2 of shapes and Fig. 6 are taken from this paper.</p> <p><strong>Shape of</strong> \(X \rightarrow (T \times d_{model})\)</p> <p><strong>Queries:</strong> \(Q = XW^Q\), Where \(W^Q\) is weight matrix introduced to calculate Q from X.</p> <p><strong>Keys:</strong> \(K = XW^K\), Where \(W^K\) is weight matrix introduced to calculate K from X.</p> <p><strong>Values:</strong> \(V = XW^V\), Where \(W^V\) is weight matrix introduced to calculate V from X.</p> <p><strong>Shape tracking</strong> → (shape of \(X\) ) Matrix Mult. (shape of \(W^Q\) ) → <strong>Output shape</strong></p> <p>\(Q = XW^Q \rightarrow (T \times d_{model}) \cdot (d_{model} \times d_k) \rightarrow (T \times d_k)\) \(K = XW^K \rightarrow (T \times d_{model}) \cdot (d_{model} \times d_k) \rightarrow (T \times d_k)\) \(V = XW^V \rightarrow (T \times d_{model}) \cdot (d_{model} \times d_v) \rightarrow (T \times d_v)\)</p> <p>Finally, substituting Q, K, V, and \(\text{const.} = \sqrt{d_k}\) (\(d_k\) dimensionality of the key vector) in above self-attention eqn.(I) , we get equation for attention as given below:</p> \[\text{Attention}(Q, K, V) = \text{SoftMax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V \quad ------ \text{(II)}\] <p>Fig.5 below is a graphical representation of obtaining Q, K, and V from input X and feeding into equation (II) above</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/self-attention/scaled_dor_product_depiction1-480.webp 480w,/assets/img/self-attention/scaled_dor_product_depiction1-800.webp 800w,/assets/img/self-attention/scaled_dor_product_depiction1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/self-attention/scaled_dor_product_depiction1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 5</figcaption> </figure> <p>Above attention eqn. (II) is also termed as Scaled Dot Product Attention depicted in Fig. 6 given below.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/self-attention/scaled_dot_product_shapes1_m-480.webp 480w,/assets/img/self-attention/scaled_dot_product_shapes1_m-800.webp 800w,/assets/img/self-attention/scaled_dot_product_shapes1_m-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/self-attention/scaled_dot_product_shapes1_m.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 6</figcaption> </figure> <p>Shape of \(QK^T\): \((T \times d_k) \times (d_k \times T) \rightarrow (T \times T)\)</p> <p>And finally, the <strong>shape of final attention output</strong>: \((T \times T) \times (T \times d_v) → (T \times d_v)\)</p> <p>If we consider a batch of \(N\) samples at a time for processing, then above shape will be: \(N \times T \times d_v\). Please note that the shapes of \(W^Q, W^K, W^V\) are chosen in a way that matrix multiplication with input <strong>‘X’</strong> is possible. The values of \(d_k, d_v , d_model\) are hyperparameters and the table of shapes above shows the values used by the author in the paper.</p> <hr/> <h3 id="database-analogy-for-queries-keys-and-values">Database Analogy for Queries, Keys, and Values</h3> <p>In the context of databases, queries are used to interact with databases to retrieve or manipulate data, keys are used to uniquely identify records and establish relationships between tables, and values are the actual data stored in the fields of a database table. The same kind of analogy exists in Self-attention Q, K, and V as well, as shown in Fig. 7 below.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/self-attention/database_inspiration-480.webp 480w,/assets/img/self-attention/database_inspiration-800.webp 800w,/assets/img/self-attention/database_inspiration-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/self-attention/database_inspiration.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 7</figcaption> </figure> <p>In this example, the word ‘check’ acts as a query, and all the words in the sequence act as keys. Attention weights find the answer to the question: ‘Which key or what are the keys that match with the query?’ by assigning different weights to the keys. Here, the word ‘check’ pays more attention to the words ‘cashed’ and ‘bank’ in the sentence, as represented by the thickness of the joining lines.</p> <p>In self-attention, every word acts as a query once, while the entire words in the sequence act as keys, and attention weights are calculated to figure out which key matches the query.</p> <p>Finally, the words (represented as vectors) are treated as values, and attention weights are used to form a weighted sum of the values, resulting in an attention vector. The attention vector for the word ‘check’ will be the weighted sum of the value vectors.</p> <hr/> <h3 id="how-self-attention-helps-in-contextual-understanding">How Self-Attention Helps in Contextual Understanding</h3> <p>In the example we looked at, we were figuring out the meaning of a word. So, if we just see the word ‘check’ by itself, it could mean different things. But when we look at the other words in the sentence, like ‘cashed’ and ‘bank’, it helps us understand that in this context ‘check’ refers to a financial document, not something like checking your homework or check in chess.</p> <blockquote> <p><strong>Notes</strong></p> <ul> <li>Every word must have an attention weight with every other word, i.e., for \(T\) number of terms, there are \(T^2\) attention computations to calculate attention weights.</li> <li>Q, K, and V are calculated independently, resulting in parallelization, unlike in RNN where h(t-1) must be computed before h(t).</li> <li>Self-attention handles long sequences better than RNNs and avoids vanishing gradients.</li> </ul> </blockquote>]]></content><author><name></name></author><category term="AIML"/><category term="AIML"/><summary type="html"><![CDATA[From intuition to scaled dot-product attention.]]></summary></entry><entry><title type="html">Transformer Model Architecture</title><link href="https://rami-rk.github.io/blog/2024/transformer-architecture/" rel="alternate" type="text/html" title="Transformer Model Architecture"/><published>2024-03-27T00:00:00+00:00</published><updated>2024-03-27T00:00:00+00:00</updated><id>https://rami-rk.github.io/blog/2024/transformer-architecture</id><content type="html" xml:base="https://rami-rk.github.io/blog/2024/transformer-architecture/"><![CDATA[<h3 id="overview">Overview</h3> <p>In my attempt to demystify the Transformer Architecture, this is the third and last article. Here, I have explained all the components and their details in a very simplified way. Please read my previous two articles: <a href="/blog/2024/birth-of-attention-mechanism/">1. Birth of Attention Mechanism</a>, and <a href="/blog/2025/self-attention/">2. Self-Attention</a>, to fully appreciate and comprehend this article. These three articles will assist you in clarifying your understanding of the transformer, which serves as the foundation for recent advancements such as BERT, GPT, ChatGPT, and LLMs. Please note that these articles are not superficial overviews but rather technical in nature.</p> <p>The Fig.1 given below shows a Transformer Model Architecture as per the paper <a href="https://arxiv.org/abs/1706.03762" target="_blank">‘Attention Is All You Need’</a>. published in 2017 by A. Vaswani et al. It has two blocks — the left block is the Encoder and the right block is the Decoder. This transformer-based encoder-decoder architecture operates similarly to the RNN-based encoder-decoder that we saw by a translation example in the previous article — <a href="/blog/2024/birth-of-attention-mechanism/">Birth of Attention Mechanism</a> but without the presence of an RNN component. Despite this difference, the input-output feeding mechanism to the encoder-decoder remains consistent. Additionally, akin to the RNN model, the decoder functions as a causal model.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/tfr_full_archi-480.webp 480w,/assets/img/transformer/tfr_full_archi-800.webp 800w,/assets/img/transformer/tfr_full_archi-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/transformer/tfr_full_archi.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 1</figcaption> </figure> <p>The encoder sends Values(V) and Queries (Q) to the decoder, linking them together to create the entire Transformer system. This connection, known as cross-attention, enables the decoder to pay attention to the encoded information from the input sequence provided by the encoder. With this mechanism, the decoder can concentrate on different sections of the input sequence when producing the output sequence.</p> <p>It’s important to understand that the encoder and decoder components can function independently as standalone models for various tasks. For instance, BERT serves as an exclusive encoder architecture, while GPT operates solely as a decoder architecture. On the other hand, T5 incorporates both encoder and decoder components within its architecture.</p> <p>The Encoder and Decoder blocks share similar components, including Positional Encoding, Multi-Head Attention, Add &amp; Norm (comprising Skip connection and Layer normalization), and Feed Forward. However, one thing to notice is that as the decoder functions as a causal model, it employs causal attention or masked attention. This means that each token is restricted to attending only to its preceding tokens, not the subsequent ones.</p> <p><strong>Mastering the Encoder block facilitates comprehension of the Decoder block and the entire Transformer architecture. Let’s see Encoder in detail.</strong></p> <hr/> <h3 id="encoder">Encoder</h3> <p>The Encoder consists of a transformer block as shown in Fig. 2. A simplified and expanded diagram is also shown on the right side. Let’s understand the internal components of a Transformer block.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/enc_unfolded-480.webp 480w,/assets/img/transformer/enc_unfolded-800.webp 800w,/assets/img/transformer/enc_unfolded-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/transformer/enc_unfolded.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 2</figcaption> </figure> <p>The <strong>Transformer block</strong> can be sub-divided into <strong>two main sub-layers</strong>:</p> <ul> <li>The first sub-layer comprises a <strong>multi-head attention mechanism</strong> that receives the queries, keys, and values as inputs.</li> <li>A second sub-layer comprises a fully connected <strong>feed-forward network</strong>.</li> </ul> <p>Following each of these two sub-layers is layer normalization, into which the sub-layer input (through a residual/skip connection) and output are fed. Regularization is also introduced into the model by applying a dropout to the output of each sub-layer (before the layer normalization step) which is not shown in the figure.</p> <p>The feed-forward network allows the model to extract higher-level features from the input. This network usually comprises two linear layers with a ReLU activation function in between. The feed-forward network allows the model to extract deeper meaning from the input data and more compactly and usefully represent the input. In the paper, an ANN with one hidden layer and a ReLu activation in the middle with no activation function at the output layer has been implemented.</p> <p>The encoder is formed by repeated transformer block joined one after another multiple times which is shown as multiplied by N in Fig.2 above. The transformer encoder is a crucial part of the transformer encoder-decoder architecture, which is widely used for natural language processing tasks.</p> <p><strong>BERT</strong> which stands for Bi-directional encoder representation is an encoder-only architecture that consists of 6 to 12 such transformer blocks and a prediction head as the last layer, shown in Fig.3 below.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/tfr_blocks_assembed-480.webp 480w,/assets/img/transformer/tfr_blocks_assembed-800.webp 800w,/assets/img/transformer/tfr_blocks_assembed-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/transformer/tfr_blocks_assembed.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 3</figcaption> </figure> <hr/> <h3 id="multi-head-attention">Multi-Head Attention</h3> <p>Please go through the previous article on the <a href="/blog/2025/self-attention/">Self-Attention</a> mechanism, which is implemented in Multi-Head Attention. In Multi-Head Attention, the Attention module repeats its computations multiple times in parallel. Each of these is called an Attention Head. The Attention module splits its Query, Key, and Value parameters N-ways and passes each split independently through a separate Head. All of these similar Attention calculations are then combined to produce a final Attention score. This is called multi-head attention and gives the Transformer greater power to encode multiple relationships and nuances for each word. A schematic representation of MHA is shown in Fig.4 below.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/mha-480.webp 480w,/assets/img/transformer/mha-800.webp 800w,/assets/img/transformer/mha-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/transformer/mha.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 4</figcaption> </figure> <p>Shape tracking and related calculations are shown with the help of Fig. 5.:</p> <ul> <li>Input shape to each attention layer : \((T \times d_{model})\)</li> <li>Output shape after each attention layer : \((T \times d_V)\)</li> <li>After concatenation, the shape of Output (concatenate them along the feature dimension) : \((T \times h d_V)\)</li> </ul> <p>Imagine that the output of each self-attention block lined up side by side after concatenation.</p> <p><strong>Final projection:</strong> \(\text{Output} = \text{Concat}(A_1, A_2, \dots, A_h) W^O\)</p> <ul> <li>Shape of : \(\text{Concat}(A_1, \dots, A_h) \rightarrow (T \times h d_V)\)</li> <li>Shape of : \(W^O \rightarrow (h d_V \times d_{model})\)</li> <li>Shape of final Output : \(\text{Concat}(A_1, A_2, \dots, A_h) W^O \rightarrow (T \times h d_V) \times (h d_V \times d_{model})\)</li> </ul> <p>\(\rightarrow (T \times d_{model})\) &gt; <strong>Back to the initial input shape.</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/mh_concat-480.webp 480w,/assets/img/transformer/mh_concat-800.webp 800w,/assets/img/transformer/mh_concat-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/transformer/mh_concat.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 5</figcaption> </figure> <p>We can notice in the above figure that each attention calculation is functioning parallelly and has no dependency on the other.</p> <hr/> <h3 id="positional-encoding">Positional Encoding</h3> <p>Passing embeddings directly into the transformer block results in missing information about the order of tokens as we get rid of RNN blocks and attention is permutation invariant i.e. order of the token does not matter to attention. Although transformers are a sequence model, it appears that this important detail has somehow been lost. Positional encoding is for rescue. Positional encoding adds positional information to the existing embeddings.</p> <p>A unique set of numbers is added at each position of the existing embeddings, such that this new set of numbers can uniquely identify which position they are located at. The following two ways are there to add positional encoding:</p> <ol> <li> <p>Positional Encoding by Sub-Classing the Embedding Layer (Trainable)</p> </li> <li> <p>Positional Encoding scheme as per the paper (non-trainable)</p> </li> </ol> <p>In the scheme suggested in the paper, the encoding is created by using a set of sins and cosines at different frequencies. The paper uses the following formula for calculating the positional encoding.</p> \[PE_{(pos,\,2i)}=\sin\!\left(\frac{pos}{10000^{2i/d_{model}}}\right)\] \[PE_{(pos,\,2i+1)}=\cos\!\left(\frac{pos}{10000^{2i/d_{model}}}\right)\] <hr/> <h3 id="causal-or-masked-attention-in-decoder-block">Causal or Masked Attention in Decoder Block</h3> <p>All the above-mentioned components are found in both the encoder and decoder blocks. However, the decoder block utilizes a Causal Attention/Masked Attention Mechanism, differing slightly from the Attention mechanism in the encoder block. Let’s explore further.</p> <p>Decoder is a causal model i.e. it acts like a text generation tool, similar to how forecasts are made in time series analysis, where it predicts the next element based on previous ones. To accomplish this, when generating output at any position in the sequence, the model should only be permitted to consider the preceding tokens, excluding any tokens that followed.</p> <p>Let’s see mathematically how to achieve this through an attention score matrix. Fig.6(a) below shows the attention score matrix for just 5 tokens for the sake of demonstration. It will have a shape of \(T \times T\) for \(T\) sequences. Here, the <strong>attention score</strong> : \(\alpha (i, j)\) represents how much \(i\) pays attention to \(j\).</p> <p>Is it possible to restrict attention weights to consider only past input tokens? Yes, through some manipulation, we can achieve this: \(\alpha (i, j) &gt; 0\) only when \(i \ge j\), resulting in the score matrix depicted in Fig.6(b).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/masking-480.webp 480w,/assets/img/transformer/masking-800.webp 800w,/assets/img/transformer/masking-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/transformer/masking.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 6</figcaption> </figure> <p>We have masked the upper triangular elements to set them to zero. Thus, rather than employing plain Multi-Head Attention, we utilize a masked version known as ‘Causal Self-Attention.</p> <p>Finally, the <strong>Decoder</strong> is depicted in Fig. 7 below, which is equivalent to the Encoder, but the transformer block implements Masked Multi-Head Attention. All <strong>GPTs(Generative Pre-trained Transformers)</strong> are versions of decoder-only architecture with many more components attached to them, trained on a humongous amount of data.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/decoder_with_masked_mha-480.webp 480w,/assets/img/transformer/decoder_with_masked_mha-800.webp 800w,/assets/img/transformer/decoder_with_masked_mha-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/transformer/decoder_with_masked_mha.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 7</figcaption> </figure> <p>We have seen both the Encoder and Decoder separately. Encoder-only architecture is BERT while Decoder-only Architecture is GPT. Similarly, the architecture that contains both Encoder and Decoder is called <strong>T5 i.e. `Text to Text Transfer Transformer’</strong>. Fig. 8 below shows how Encoder and Decoder are connected in full Encoder and Decoder architecture.</p> <div style="text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/enc_dec_connection-480.webp 480w,/assets/img/transformer/enc_dec_connection-800.webp 800w,/assets/img/transformer/enc_dec_connection-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/transformer/enc_dec_connection.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 50%; " data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 8</figcaption> </figure> </div> <hr/> <h3 id="why-does-multi-head-attention-work">Why Does Multi-Head Attention Work</h3> <p>It allows the layer to learn multiple features. Think of attention outputs as features that tell us something about the sentence. For example, see Fig. 9 below, when Suman pays attention to bank, the feature might be Where did Suman go? But we might also be interested in what Suman did. He cashed the check. We can only do this if we allow our layer to produce multiple features, each of which has the Suman token pay attention to different tokens in the sentence. There would be multiple and different kind of relationships and dependencies between different tokens in any sequence. Multi-head attention helps in learning those different relationships by learning corresponding multiple features.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/intuition_mha-480.webp 480w,/assets/img/transformer/intuition_mha-800.webp 800w,/assets/img/transformer/intuition_mha-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/transformer/intuition_mha.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 9</figcaption> </figure>]]></content><author><name></name></author><category term="AIML"/><category term="AIML"/><summary type="html"><![CDATA[From encoders to masked attention and decoders - Transformer architecture demystified.]]></summary></entry><entry><title type="html">Birth of Attention Mechanism</title><link href="https://rami-rk.github.io/blog/2024/birth-of-attention-mechanism/" rel="alternate" type="text/html" title="Birth of Attention Mechanism"/><published>2024-03-26T00:00:00+00:00</published><updated>2024-03-26T00:00:00+00:00</updated><id>https://rami-rk.github.io/blog/2024/birth-of-attention-mechanism</id><content type="html" xml:base="https://rami-rk.github.io/blog/2024/birth-of-attention-mechanism/"><![CDATA[<h3 id="overview">Overview</h3> <p>The onset of ChatGPT and many other open-source LLMs have revolutionized the world and the way work happens in all sorts of industries. But very few of us know that transformer-based models are the mother of all these magic tools- including BERT, GPT, and all the buzzwords around large language models.</p> <p>I have struggled to understand the underlying concept and the working principle of <strong>Transformer</strong>, and I believe many struggle in the same way.</p> <p>I am going to publish a series of articles on the Transformer to simplify the concept and break down things into small pieces. I believe this will help many to understand these complex concepts easily and help them to break into this domain.</p> <p>To understand the transformer architecture, first, we have to understand the attention mechanism- the main component of transformer architecture and the concept of self-attention, and finally, the full Transformer Architecture. Maintaining the conceptual logic above, the following are the three articles that I am going to publish. The first article in the series is <strong>“The Birth of Attention Mechanism”</strong>.The second article in the series is <strong>“Self-Attention”</strong>, and the third article is <strong>“Transformer Model Archetecture”</strong>.</p> <p>This is the first article and we are going to understand the attention mechanism in the context of language translation using seq2seq (encoder-decoder) architecture. In the example below, a Hindi sentence <strong>“मुझे खाना पसंद है”</strong> is fed into an encoder which has been translated(decoded) by decoder into English as <strong>“I love to eat”</strong>.</p> <p>The encoder is used to ingest the input. It simply computes all the \(h \text{ of T’s} (\text{ i.e. } h(1), h(2), …)\) at every time step of the input and gives one final hidden state vector, which we’ll call \(h(T)\), assuming that the input sequence has a length of \(T\).</p> <p>Here, \(h(T)\) is the output of the encoder, which is also the input to the decoder. We can think of \(h(T)\) as a vector, a compressed representation of the input sequence.</p> <blockquote> <p>\(h(T)\): A compressed representation of the input. → “Thought Vector”</p> </blockquote> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/birth-of-attention-mechanism/enc_dec_rnn-480.webp 480w,/assets/img/birth-of-attention-mechanism/enc_dec_rnn-800.webp 800w,/assets/img/birth-of-attention-mechanism/enc_dec_rnn-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/birth-of-attention-mechanism/enc_dec_rnn.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 1</figcaption> </figure> <hr/> <h3 id="lets-see-the-decoder-in-detail">Lets See the Decoder in Detail</h3> <p>Every RNN unit has two sources of input data, the first being the actual input and the second being the previous head state. The decoder is also an RNN but a different RNN from the encoder. In this diagram above, the inputs go along the bottom of the RNN unit while the previous head state comes through the left. For a normal RNN, the initial hidden state is usually just a vector of zeros, or maybe some trainable but fixed vector. In this case, for the decoder, the initial hidden state comes from the output of the encoder, which is \(h(T)\). Thus, the job of the decoder is to decompress the compressed representation of the input.</p> <hr/> <h3 id="what-goes-into-the-bottom-of-the-decoder-rnn-unit">What Goes Into the Bottom of the Decoder RNN Unit</h3> <p>At the beginning, there’s a special token called \(\text{&lt;SOS&gt;}\) which indicates the start of a sentence or sequence. The decoder RNN acts like a text generation tool, similar to how forecasts are made in time series analysis, where it predicts the next element based on previous ones, such a model is also known as an autoregressive model or causal model. So, starting with the \(\text{&lt;SOS&gt;}\) token and an initial hidden state, we predict the first word of the translated sequence. After getting this first word, we put it back into the decoder as input, using it to predict the second word in the sequence. We keep repeating this process until we’ve generated the entire translated sequence. To know when to stop, we watch for the special end token produced by our model, signaling that the translation is complete.</p> <p>Note that, we need a lot of data sets containing paired Hindi sentences and equivalent English sentences to train the model, and only after the model performs the translation as shown in the diagram above.</p> <hr/> <h3 id="problem-with-seq2seq">Problem With Seq2Seq</h3> <ol> <li>Every input sequence is converted using an encoder RNN into a single vector. Here’s the problem — What if the input sentence is very long? The final encoder state \(h(T)\) always has the same size and has to remember the whole input.</li> <li>Humans do not translate by remembering entire sentences at once; we focus on relevant parts as we go.</li> </ol> <hr/> <blockquote> <p><strong>To Solve This Problem: Attention is Originated for Seq2Seq</strong></p> </blockquote> <div style="text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/birth-of-attention-mechanism/attention_def-480.webp 480w,/assets/img/birth-of-attention-mechanism/attention_def-800.webp 800w,/assets/img/birth-of-attention-mechanism/attention_def-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/birth-of-attention-mechanism/attention_def.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 70%; " data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 2</figcaption> </figure> </div> <p>Let’s try to understand this attention concept with a simple example, where the input: <strong>“मेँ खेलने जा रहा हूँ “</strong> fed into an encoder and translated into “I am going to play” as an output from the decoder as shown in fig.3 below. In this figure ‘i/p’ at the bottom represents the input sequence and <strong>‘o/p’</strong> at the top represents the output sequence.</p> <p>Here, the list ‘t1’ contains the values for attention that output at time step 1 should pay attention to all input sequences. The list <strong>‘t2’</strong> contains the values for attention that output at time step 2 should pay attention to all input sequences and similarly other lists also represent attention values for input sequences for different output at different time steps.</p> <div style="text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/birth-of-attention-mechanism/attention_values-480.webp 480w,/assets/img/birth-of-attention-mechanism/attention_values-800.webp 800w,/assets/img/birth-of-attention-mechanism/attention_values-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/birth-of-attention-mechanism/attention_values.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 70%; " data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 3</figcaption> </figure> </div> <p>Let’s understand the attention values inside list <strong>‘t1’</strong>. When we’re aiming to generate the first word ‘I’ in our output, what we’re doing is figuring out the likelihood of each input word being the one we should focus on. At this stage, it’s fine if we don’t know the translations for <strong>‘खेलने’</strong> or <strong>‘जा रह’</strong> or <strong>‘हू’</strong>, as long as we know the translation for <strong>‘मेँ’</strong> because that’s the word we need to generate first. So, we can simplify things by saying that, at this point, we only need to pay attention to the first word in the input and can ignore everything else. This is the reason the attention value is 1 for the first input (मेँ) and 0 for all other input sequences.</p> <p>What about the second time step word <strong>“am”</strong> in output? We just need to focus on the last word हूँ in the input, and thus, the list ‘t2’ contains attention value 1 at last and 0 for all other input sequences.</p> <p>What about the third time step word <strong>“going”</strong> in output? Is it always going to be that we only need to focus on one word at a time? No. We will focus on two input sequences <strong>‘जा‘</strong> and <strong>‘रहा’</strong> and ignore everything else. This is the reason, the list t3 contains attention values 0.5 and 0.5 at respective time step positions of <strong>‘जा‘</strong> and <strong>‘रहा’</strong> and the remaining all are zero. Similarly, for the remaining time step.</p> <p>Is this the encoder-decoder architecture doing, that we saw previously in Fig.1? No! Every time step focuses on the encoding of the entire sentence (i.e. final h(T)) because that is the encoding that we are feeding to every time step. This is the problem we need to fix.</p> <p>We need to learn to pay attention to certain important parts of the sentence. Ideally, at each time step, we should feed only the relevant information i.e. encodings of relevant words to the decoder.</p> <hr/> <h3 id="improved-encoder-decoder-architecture-with-attention">Improved Encoder-Decoder Architecture With Attention</h3> <p>Let’s see how to improve the previous encoder-decoder architecture with the concept of Attention.</p> <p>In the given Fig.3 below, the decoder is drawn on top of the encoder and instead of feeding only the last hidden state of the encoder to the decoder all hidden states are fed into the decoder at each time step with some weightage noted by alpha. For the sake of simplicity, connection for only output at time state \(t=2 \text{ and } t=4\) have shown.</p> <p>A weighted combination of each output is taken from the encoder cells (i.e., vector of the same shape) and fed into each step cell of the decoder. Assume for the time being that, we got these alpha’s (i.e. attention weights for each hidden state from the encoder) from some magic, we will figure out how to calculate it.:</p> <blockquote> <p><strong>For output at the time state t=2 ie. ‘Love’:</strong> \(\alpha_{2,1} \text{ is the weight of attention that is should pay to input at time state 1}\) \(\alpha_{2,2} \text{ is the weight of attention that is should pay to input at time state 2}\) \(\alpha_{2,3} \text{ is the weight of attention that is should pay to input at time state 3}\) \(\alpha_{2,4} \text{ is the weight of attention that is should pay to input at time state 4}\)</p> </blockquote> <blockquote> <p><strong>Similarly, for output at time step t = 4 i.e. ‘eat’:</strong> \(\alpha_{4,1} \text{ is the weight of attention that is should pay to input at time state 1}\) , and similarly remainings: \(\alpha_{4,2}, \alpha_{4,3}, \text{ and }\alpha_{4,4}\)</p> </blockquote> <div style="text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/birth-of-attention-mechanism/rnn_with_attention-480.webp 480w,/assets/img/birth-of-attention-mechanism/rnn_with_attention-800.webp 800w,/assets/img/birth-of-attention-mechanism/rnn_with_attention-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/birth-of-attention-mechanism/rnn_with_attention.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 70%; " data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 4</figcaption> </figure> </div> <hr/> <h3 id="context-vector">Context Vector</h3> <p>Now, defining weighted combination as a context vector, say for output at time step 4:</p> \[C(4) = \alpha_{4,1} h(1) + \alpha_{4,2} h(2) + \alpha_{4,3} h(3) + \alpha_{4,4} h(4)\] <p>Denoting :</p> <ul> <li>Input time step by j = 1, 2 ,.., T</li> <li>Output time step by i = 1, 2 ,.., t</li> </ul> <p>We can generalize the above equation as a <strong>Context Vector</strong>:</p> \[C(i) = \sum_{j=1}^{T_x} \alpha_{i,j} h(j)\] <p>Here, the context vector is simply the weighted sum of h’s where the weights are the alphas. So, this is the basic idea behind the attention. We have weights from every input to every output, which tells us how much each output should pay attention to which input.</p> <hr/> <h3 id="calculating-attention-weights">Calculating Attention Weights</h3> <p>Before calculating alphas, let’s introduce a term called <strong>alignment score</strong> noted by $e_{i,j}$ and defined as: at <strong>i<sup>th</sup></strong> time step of decoder, how important is the <strong>j<sup>th</sup></strong> word in the input. This should depend on or should be a function of j<sup>th</sup> word and <strong>whatever has happened in the decoder</strong> so far. Note, S₁, S₂…, Sₜ are decoder side hidden states.</p> \[score: e_{i,j} = f(s_{i-1}, h_j)\] <p>We are interested in $e_{i,j}$ for all the input words. For <strong>j<sup>th</sup></strong> input word, we are interested in knowing how important it is for the <strong>i<sup>th</sup></strong> time-step output of the decoder. <strong>There are several ways this function can be defined and no need to know at this moment but the intuition that it depends on $s_{i-1}$, and $h_j$ is sufficient.</strong></p> <p>Now across all the input words, we want the sum of this score value to be one. We don’t want some arbitrary weights. It is just like probability distribution over what word is important by how much. We can achieve this by taking <strong>SoftMax of the score</strong>.</p> \[\alpha_{i,j} = \frac{exp(e_{i,j})}{\sum_{k=1}^{T} exp(e_{i,k})}\] <p><strong>Attention Weight, $\alpha_{i,j}$</strong> denotes the probability of focusing on the <strong>j<sup>th</sup></strong> word to produce <strong>i<sup>th</sup></strong> output word.</p>]]></content><author><name></name></author><category term="AIML"/><category term="AIML"/><summary type="html"><![CDATA[The evolution of attention in sequence-to-sequence models.]]></summary></entry></feed>