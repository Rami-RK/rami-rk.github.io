<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://rami-rk.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://rami-rk.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-26T11:40:06+00:00</updated><id>https://rami-rk.github.io/feed.xml</id><title type="html">blank</title><subtitle>Demystifying AI, Machine Learning, and Deep Learning concepts. Articles on Transformers, Attention Mechanisms, Neural Networks, and Mathematics for ML. </subtitle><entry><title type="html">Continuous Distribution &amp;amp; PDF</title><link href="https://rami-rk.github.io/blog/2025/continuous-distribution/" rel="alternate" type="text/html" title="Continuous Distribution &amp;amp; PDF"/><published>2025-05-03T00:00:00+00:00</published><updated>2025-05-03T00:00:00+00:00</updated><id>https://rami-rk.github.io/blog/2025/continuous-distribution</id><content type="html" xml:base="https://rami-rk.github.io/blog/2025/continuous-distribution/"><![CDATA[<p>We have discussed about discrete distributions in previous article and in discrete distributions, the events always form a list. For example, if I roll a die, I could get 1, 2, 3, 4, 5, or 6. I could also think of the number of books in a library; it can be 1, 2, 3, a thousand, etc., but I can always make a list of it.</p> <p>What is something that cannot be listed? An interval. For example, if my random variable is the actual length of a manufactured shaft, that cannot be listed. Because the nominal length of the shaft is 100 mm, but it could also be 99.91 mm, or 100.01 mm, or 100.001 mm. Actually, it depends on the precision of the measuring device and up to what decimal digits we are measuring the accuracy, resulting in infinitely many values. However, we can define an interval and say that the shaft length lies between 99.90 mm and 100.10 mm.</p> <p>So, when your events are a list, you have a discrete distribution. And when your events are an interval, you have a continuous distribution. Let’s elaborate more on that.</p> <p>So, imagine you’re measuring the length of different shafts and you’re trying to find the probability that a shaft will have a specific length. Let’s say the length could be exactly 99.90 mm, 100.00 mm, or 100.10 mm, for example. And we can try to plot the probabilities, say like this, where the heights of the bars are the probabilities that the length is exactly 99.90 mm, 100.00 mm, or 100.10 mm.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/random_variable_and_distribution/probability_bar-480.webp 480w,/assets/img/random_variable_and_distribution/probability_bar-800.webp 800w,/assets/img/random_variable_and_distribution/probability_bar-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/random_variable_and_distribution/probability_bar.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 1</figcaption> </figure> <p>But it can also be 99.91 mm, or 100.05 mm. And you can quickly see that there are infinitely many values the length could take, as values with three or more decimal places are also possible. These values lie everywhere between the ones you already have, both to the left and to the right.</p> <p>Since height of the bars are representing probabilities the sum of the heights of all of them has to be equal to 1. But you can quickly see that by adding more and more and more of them, they have to get smaller and smaller and eventually become 0. So, what did we do wrong here? Well, we did nothing wrong. The answer is that this distribution is fundamentally different, as it’s not discrete, but it is continuous. So this approach will not really work.</p> <p>To try to understand this, think of how you would answer the following question: what is the probability that the length will be exactly 100.00 mm to the dot? It is encouraged to think about this. The answer is zero, because there are simply too many values for the length. There’s actually uncountably many of them; it’s a whole interval. And we’re forced to say that the probability that the length is exactly 100.00 and forever 0 mm is 0. So, we need to describe this problem in a slightly different way.</p> <p>Instead of asking what is the probability of the length being a fixed value, let’s think of it in terms of ranges. So, we’re thinking of what’s the probability that the length falls within a certain range, say between 99.90 and 99.95 mm. And for that, we can have a probability, and we can put it as the height of this red bar over here. Then we can think of the probability that the length falls between 99.95 and 100.00 mm. And that’s this other height, or between 100.00 and 100.05 mm, and so on.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/random_variable_and_distribution/probability_bar_range-480.webp 480w,/assets/img/random_variable_and_distribution/probability_bar_range-800.webp 800w,/assets/img/random_variable_and_distribution/probability_bar_range-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/random_variable_and_distribution/probability_bar_range.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 2</figcaption> </figure> <p>And let’s assume that we are only considering lengths between 99.90 and 100.10 mm. And now we have a discrete probability distribution, where the areas, so the sum of the heights of these bars, adds up to 1. And notice that the majority of the lengths fall between 99.95 and 100.00 mm, or between 100.00 and 100.05 mm; not many of them go all the way to 100.10 mm, and that’s what this probability distribution is telling us.</p> <p>Now imagine that we want a little more information, so we make these ranges a little more granular, so we make them 0.01 mm instead. And now we have this distribution over here, where we now know the probability that the length is between 99.90 and 99.91 mm, between 99.91 and 99.92 mm, all the way to between 100.09 and 100.10 mm.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/random_variable_and_distribution/grannular_probability_bar-480.webp 480w,/assets/img/random_variable_and_distribution/grannular_probability_bar-800.webp 800w,/assets/img/random_variable_and_distribution/grannular_probability_bar-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/random_variable_and_distribution/grannular_probability_bar.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 3</figcaption> </figure> <p>And if we want to make them still more granular, we can make the ranges even smaller, like 0.005 mm. And now we have all this information between 99.90 and 99.905 mm, between 99.905 and 99.91 mm, all the way to between 100.095 and 100.10 mm. And we can continue splitting these ranges and getting discrete distributions. And if we were to do this infinitely many times, then what we get is this: a continuous distribution. Imagine just a bunch of very, very skinny bars, infinitely many of them, that become just a curve.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/random_variable_and_distribution/continuous_distribution-480.webp 480w,/assets/img/random_variable_and_distribution/continuous_distribution-800.webp 800w,/assets/img/random_variable_and_distribution/continuous_distribution-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/random_variable_and_distribution/continuous_distribution.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 4</figcaption> </figure> <p>Now, in the discrete distribution, we had that the sum of heights has to be equal to 1. The sum of heights was the same as saying the blue area. So, in the continuous distribution, we have the same condition: the area under the curve is equal to 1. And that is a continuous probability distribution.</p> <hr/> <h3 id="probability-density-function">Probability Density Function</h3> <p>Let’s continue with same example of shaft length but with more concrete values and samples with length measurements. The nominal length of a shaft is 100 mm, with manufacturing tolerance is 0.1 mm, the actual length will be within the range of 100 ± 0.1 mm. The length measured vary from 99.90 mm to 100.10 mm. 100 sample measurements of the length are given in Table. It ranges from 99.90 mm to 100.10 mm, certain values occur more frequently than others. The values around the nominal length 100 mm occur with a higher chance than the values near both endpoints.</p> <p><strong>Table: Length of 100 Samples of shaft</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/random_variable_and_distribution/shaft_data-480.webp 480w,/assets/img/random_variable_and_distribution/shaft_data-800.webp 800w,/assets/img/random_variable_and_distribution/shaft_data-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/random_variable_and_distribution/shaft_data.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>If we divide the range [99.90, 100.10] into several equal segments and plot the number of values of the length that reside the segments, we will have a bar-like graph. This type of graph is called a <strong>histogram</strong>. It shows the frequency of the values that occur in different segments.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/random_variable_and_distribution/histogram_with_number_of_samples-480.webp 480w,/assets/img/random_variable_and_distribution/histogram_with_number_of_samples-800.webp 800w,/assets/img/random_variable_and_distribution/histogram_with_number_of_samples-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/random_variable_and_distribution/histogram_with_number_of_samples.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 5</figcaption> </figure> <p>If we plot the number of samples (measurements) divided by the total number of measurements, we obtain a variant of the last histogram. As shown in Fig. below, the vertical axis represents the number of measurements within each segment divided by the total number of measurements (100).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/random_variable_and_distribution/histogram-480.webp 480w,/assets/img/random_variable_and_distribution/histogram-800.webp 800w,/assets/img/random_variable_and_distribution/histogram-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/random_variable_and_distribution/histogram.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 6</figcaption> </figure> <p>If we have more samples and use more intervals to divide the range of the length, the bars in the last Fig. will approach a smooth curve as shown in Fig. below. This curve is called a probability density function (pdf).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/random_variable_and_distribution/probability_desnsity_fn-480.webp 480w,/assets/img/random_variable_and_distribution/probability_desnsity_fn-800.webp 800w,/assets/img/random_variable_and_distribution/probability_desnsity_fn-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/random_variable_and_distribution/probability_desnsity_fn.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 7</figcaption> </figure> <p>The pdf captures the chance property of a random variable as shown in Fig. below and fully describes a random variable.</p> <ul> <li>\(f(x)\) is used the denote a probability density function of random variable \(X\), where \(x\) is a realization (a specific value) of \(X\).</li> <li>The significance of the pdf is that \(f(x)dx\) is the probability that the random variable \(X\) is in the interval \([x, x + dx]\) (see Fig below), written as: \(P(x \leq X \leq x + dx) = f(x) \, dx\)</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/random_variable_and_distribution/continuous_distribution_def-480.webp 480w,/assets/img/random_variable_and_distribution/continuous_distribution_def-800.webp 800w,/assets/img/random_variable_and_distribution/continuous_distribution_def-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/random_variable_and_distribution/continuous_distribution_def.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 8</figcaption> </figure> <p>We can also determine the probability of \(X\) over a finite interval \([a, b]\) as:</p> \[P(a \leq X \leq b) = \int_a^b f(x) \, dx\] <p>, also shown in figure above, which is the area underneath the curve of \(f(x)\) from \(x = a\) to \(x = b\).</p> <p>A pdf must be non-negative, i.e. \(f(x) \geq 0\) and satisfies the following condition:</p> \[\int_{-\infty}^{+\infty} f(x) \, dx = 1\] <hr/> <h3 id="example">Example</h3> <p>Suppose the length of metal rods produced by a manufacturer follows a continuous distribution and probability density function (pdf) is given by:</p> \[f(x) = \begin{cases} \dfrac{1}{2}, &amp; \text{for } 99 \le x \le 101 \\ 0, &amp; \text{otherwise} \end{cases}\] <p>Determine the probability that the rod’s length is between 99.5 mm and 100.5 mm.</p> <p><strong>Solution:</strong></p> <p>The probability is the area under the pdf curve between the specified limits as given.</p> \[P(99.5 \leq X \leq 100.5) = f(x) \cdot dx = f(x) \times (100.5 - 99.5)\] <p>Since \(f(x) = 1/2\)</p> \[P(99.5 \leq X \leq 100.5) = f(x) \times (100.5 - 99.5) = \frac{1}{2} \times 1 = 0.5\] <p><strong>Conclusion:</strong> There is a 50% probability that a randomly selected rod will have a length between 99.5 mm and 100.5 mm.</p> <p><strong>One observation:</strong> In this example the density function remains constant for the specified ranges, for example PDF value is half for \(99 \leq x \leq 101\). This is an example of uniform distribution, a type of continuous distribution.</p> <p><strong>Normal distribution, Exponential Distribution, and Uniform Distribution are examples of Continuous Probability Distributions. In the next article, we will focus on learning about the Normal distribution.</strong></p>]]></content><author><name></name></author><category term="Math"/><category term="Math"/><category term="probability"/><category term="statistics"/><summary type="html"><![CDATA[Understand continuous probability distributions and PDF. Learn how histograms become density functions with intuitive manufacturing examples.]]></summary></entry><entry><title type="html">Normal Distribution</title><link href="https://rami-rk.github.io/blog/2025/normal-distribution/" rel="alternate" type="text/html" title="Normal Distribution"/><published>2025-05-02T00:00:00+00:00</published><updated>2025-05-02T00:00:00+00:00</updated><id>https://rami-rk.github.io/blog/2025/normal-distribution</id><content type="html" xml:base="https://rami-rk.github.io/blog/2025/normal-distribution/"><![CDATA[<p>Let’s continue with the case study of shaft length that we saw in the continuous distribution explanation. We were given lengths of 100 samples of shafts with a manufacturing tolerance of 0.1 mm, meaning the actual length will be within the range of 100 ± 0.1 mm. We divided the range [99.90, 100.10] into several equal segments and plotted the number of length values that reside in these segments. We had a bar-like graph called a histogram. It shows the frequency of the values that occur in different segments. We further modified it by dividing the frequency within each segment by the total sample count, which is 100. The final plot is shown below:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/random_variable_and_distribution/histogram-480.webp 480w,/assets/img/random_variable_and_distribution/histogram-800.webp 800w,/assets/img/random_variable_and_distribution/histogram-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/random_variable_and_distribution/histogram.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 1</figcaption> </figure> <p>We can approximate this distribution with the bell-shaped curve shown below, and we will call this distribution as following Normal distribution.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/random_variable_and_distribution/histogram_with_bell_curve-480.webp 480w,/assets/img/random_variable_and_distribution/histogram_with_bell_curve-800.webp 800w,/assets/img/random_variable_and_distribution/histogram_with_bell_curve-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/random_variable_and_distribution/histogram_with_bell_curve.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 2</figcaption> </figure> <p>Now observe that most of the sample lengths are centered around a mean of approximately 100 mm and spread on both sides almost symmetrically. The ideal Normal distribution assumes a bell-shaped curve that is symmetrical around the mean value with a certain variance.</p> <p>Normal random variables, also often called Gaussian random variables, are perhaps the most important ones in probability theory. They are prevalent in applications for two reasons: they have useful analytical properties, and they are the most common model for random noise. In general, they are a good model of noise or randomness whenever that noise is due to the addition of many small independent noise terms, and this is a very common situation in the real world.</p> <p>We define normal random variables by specifying their PDFs as they are continuous distributions. We start with the simplest case, known as standard normal. The standard normal is denoted using the shorthand notation given below, and we will see shortly why this notation is used.</p> <hr/> <h3 id="standard-normal">Standard Normal</h3> \[\mathcal{N}(0,1): \quad f_x(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}\] <p>This PDF is defined for all values of x, meaning x can be any real number i.e. this random variable can take values anywhere on the real line. Let us try to understand this formula.</p> <p>It includes the exponential of \(-\frac{x^2}{2}\) (negative x squared over 2). If we are to plot the function \(\frac{x^2}{2}\), it has a shape of the form shown below left, and it is centered at zero.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/random_variable_and_distribution/standard_normal_distribution-480.webp 480w,/assets/img/random_variable_and_distribution/standard_normal_distribution-800.webp 800w,/assets/img/random_variable_and_distribution/standard_normal_distribution-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/random_variable_and_distribution/standard_normal_distribution.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 3</figcaption> </figure> <p>But then we take the negative exponential of this function. When you take the negative exponential, the value becomes small whenever \(\frac{x^2}{2}\) is large. Therefore, the negative exponential equals 1 when x is 0. As x increases and \(x^2\) also increases, the negative exponential decreases. Thus, we obtain a shape like the one shown on the top right in red colour, which is symmetrical on both sides.</p> <p>Finally, there is the constant \(\frac{1}{\sqrt{2\pi}}\): Where is this constant coming from? Well, there is a useful but somewhat complex calculus exercise that shows that the integral from minus infinity to plus infinity of \(e^{-\frac{x^2}{2}}\) is equal to \(\sqrt{2\pi}\).</p> <p>I.e. \(\int_{-\infty}^{+\infty} e^{-\frac{x^2}{2}} \, dx = \sqrt{2\pi}\)</p> <p>Now, we need a PDF that integrates to 1, meaning the area under the PDF curve should be 1. To achieve this, we include this constant in front of the expression so that the integral equals 1. This explains the presence of this particular constant.</p> <p><strong>What is the mean of this random variable?</strong></p> <p>Since \(x^2\) is symmetric around 0, and for this reason, the PDF itself is symmetric around 0. Therefore, by symmetry, the mean has to be equal to 0 or expectation: \(E[X] = 0\). This explains the entry zero in first position of standard normal notation: \(\mathcal{N}(0,1)\).</p> <p><strong>How about the variance?</strong></p> <p>To calculate the variance, you need to solve a calculus problem involving integration by parts. After completing the calculation, you find that the variance is equal to 1 or \(\text{var}(X) = 1\). This explains the entry one in the notation of standard normal: \(\mathcal{N}(0,1)\).</p> <hr/> <h3 id="normal-random-variable">Normal Random Variable</h3> <p><strong>General Normal (Gaussian) random variables</strong></p> <p>Let us now define general normal random variables. General normal random variables are specified their corresponding PDF as given below, which is more complex, and involves two parameters: \(\mu\), and \(\sigma^2\), where sigma is a positive parameter, \(\sigma &gt; 0\).</p> \[\mathcal{N}(\mu, \sigma^2): \quad f_x(x) = \frac{1}{\sigma \sqrt{2\pi}} \, e^{-\frac{(x-\mu)^2}{2\sigma^2}}\] <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/random_variable_and_distribution/normal_distribution-480.webp 480w,/assets/img/random_variable_and_distribution/normal_distribution-800.webp 800w,/assets/img/random_variable_and_distribution/normal_distribution-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/random_variable_and_distribution/normal_distribution.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 4</figcaption> </figure> <p>Once again, it will have a bell shape, but this bell is no longer symmetric around 0, and its width can be controlled.</p> <p>To understand the form of this PDF, let’s first focus on the exponent, just as we did with the standard normal case. The exponent is quadratic and is centered at \(x = \mu\). It vanishes when \(x = \mu\) and becomes positive elsewhere. The curve is shown below in red. Taking the negative exponential of this quadratic results in a function that is highest at \(x = \mu\) and decreases as we move further away from \(\mu\) shown by purple colour.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/random_variable_and_distribution/normal_distribution2-480.webp 480w,/assets/img/random_variable_and_distribution/normal_distribution2-800.webp 800w,/assets/img/random_variable_and_distribution/normal_distribution2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/random_variable_and_distribution/normal_distribution2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 5</figcaption> </figure> <p><strong>What is the mean of this random variable?</strong></p> <p>Since the exponent term is symmetric around \(\mu\), the PDF is also symmetric around \(\mu\), and therefore, the mean is also equal to \(\mu\), i.e. \(E[X] = \mu\).</p> <p><strong>What about the variance?</strong></p> <p>It turns out– and this is a calculus exercise that we will omit– that the variance of this PDF is equal to \(\sigma^2\), i.e. \(\text{var}(x) = \sigma^2\).</p> <p>This explains the notation: \(\mathcal{N}(\mu, \sigma^2)\), which indicates that we are dealing with a normal distribution with a mean of \(\mu\), and a variance of \(\sigma^2\).</p> <hr/> <h3 id="role-of-sigma">Role of Sigma</h3> <p>We mentioned that in a general normal distribution, the width of the bell curve can be controlled, and this is the role of \(\sigma\) in the PDF. The figure below shows three normal curves with the same mean value of 0.5 but different \(\sigma\) values of 0.25, 0.5, and 0.75, respectively. We can clearly see that as \(\sigma\) increases, the width of the curve also increases.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/random_variable_and_distribution/role_of_sigma-480.webp 480w,/assets/img/random_variable_and_distribution/role_of_sigma-800.webp 800w,/assets/img/random_variable_and_distribution/role_of_sigma-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/random_variable_and_distribution/role_of_sigma.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 6</figcaption> </figure> <hr/> <h3 id="examples-of-normal-distribution">Examples of Normal Distribution</h3> <p>Many processes in nature result in events that follow a normal distribution. Height, weight, Test Scores, and even Measurement Errors are modeled by the normal distribution. In general, variables that can be modeled as a sum of many independent processes always follow normal distributions.</p> <p>However, not all natural processes follow a normal distribution; some may follow other distributions depending on the underlying factors and their interactions viz. Earthquakes, Population Growth, Disease Outbreaks etc don’t follow normal distributions.</p> <hr/> <h3 id="how-does-it-fit-into-learning-aiml">How does it fit into learning AIML?</h3> <p>Many machine learning algorithms have underlying assumptions of normal distributions. For example, Linear Regression and Logistic Regression assume normally distributed residuals and log-odds, respectively, for accurate inference. Gaussian Naive Bayes relies on the normal distribution for feature classification. The Central Limit Theorem supports using the normal distribution for sampling means. Techniques such as z-score normalization in feature scaling, and Gaussian Mixture Models for clustering often assume normality.</p> <p>Additionally, neural networks use the normal distribution for weight initialization and batch normalization, while anomaly detection utilizes Gaussian models to identify deviations from normal behavior.</p> <p>Thus, the normal distribution is pervasive in AI and ML, often operating behind the scenes.</p>]]></content><author><name></name></author><category term="Math"/><category term="Math"/><category term="probability"/><category term="statistics"/><summary type="html"><![CDATA[Complete guide to Normal (Gaussian) distribution. Learn standard normal, mean, variance, sigma, and why it matters for machine learning algorithms.]]></summary></entry><entry><title type="html">Random Variables</title><link href="https://rami-rk.github.io/blog/2025/random-variables/" rel="alternate" type="text/html" title="Random Variables"/><published>2025-05-01T00:00:00+00:00</published><updated>2025-05-01T00:00:00+00:00</updated><id>https://rami-rk.github.io/blog/2025/random-variables</id><content type="html" xml:base="https://rami-rk.github.io/blog/2025/random-variables/"><![CDATA[<p>We will see the formal definition of random variables later. First, let’s start with a simple <strong>experiment</strong> of tossing a coin.</p> <p>Tossing a coin results in either a head or a tail. Here, each possible outcome (getting a head or getting a tail) is an <strong>event</strong>. For a fair coin, the probability of getting a head or a tail is one-half.</p> <p>\(P(H) = 0.5\) and \(P(T) = 0.5\)</p> <p>Let’s take a variable \(X\) and call it the number of heads. When we flip a coin, if we obtain heads, we get 1 head, and if we obtain tails, we get 0 heads.</p> <p><strong>X = Number of heads</strong></p> <p>Now writing the same probability of getting tail and head in this new framing of X as illustrated in Fig.1:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/random_variable_and_distribution/coin_flip-480.webp 480w,/assets/img/random_variable_and_distribution/coin_flip-800.webp 800w,/assets/img/random_variable_and_distribution/coin_flip-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/random_variable_and_distribution/coin_flip.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 1</figcaption> </figure> <p>The probability of \(X = 1\) is the same as the probability of obtaining heads: \(P(X=1) = 0.5\)</p> <p>The probability of \(X = 0\) is the same as the probability of obtaining tails: \(P(X=0) = 0.5\)</p> <p>Here, <strong>X</strong> is a random variable as it can take either the value 1 or 0 randomly, with each value occurring roughly half of the time. Symbolically all possible outcomes/values that random variable can take are denoted by small \(x\), so \(x_1 = 1, x_2 = 0\). We can simply write \(X = \{x_1, x_2\} = \{1, 0\}\). There are only two values in <strong>sample space</strong>, \(\Omega = \{1, 0\}\).</p> <p><strong>Note:</strong> \(P(X = x)\) is read as <strong>“the probability that the random variable \(X\) equals \(x\)“</strong> and \(P(X = x) = 1/2\) for either \(x = 0\) or \(1\).</p> <p><strong>In simpler terms, a random variable X assigns a real number to each outcome in the sample space \(\Omega\).</strong></p> <hr/> <h3 id="rolling-a-fair-six-sided-die">Rolling a Fair Six-Sided Die</h3> <p>The sample space consists of all possible outcomes when rolling a six-sided die: \(\Omega = \{1,2,3,4,5,6\}\).</p> <p>Since the die is fair, each outcome is equally likely. Thus, the probability distribution of the random variable \(X\) is \(P(X = x) = 1/6\) for \(x \in \{1,2,3,4,5,6\}\).</p> <hr/> <h3 id="three-fair-coin-tosses">Three Fair Coin Tosses</h3> <p>Define a random variable, X = Number of heads in 3-coin tosses.</p> <p><strong>Examine possible outcomes:</strong></p> <ul> <li>All three coins land on the tail: \((ttt)\) Number of heads \((x) = 0\)</li> <li>At least one coin lands on the head and it can happen in 3 ways: \((tth, tht, htt)\) Number of heads \((x) = 1\)</li> <li>Two coins land on the head and it can happen in 3 ways: \((thh, hth, hht)\) Number of heads \((x) = 2\)</li> <li>All three coins land on the head: \((hhh)\) Number of heads \((x) = 3\)</li> </ul> <table> <tbody> <tr> <td>Sample space, $$ \Omega = {ttt, tth, tht, htt, thh, hth, hht, hhh}; \quad</td> <td>\Omega</td> <td>= 8 $$</td> </tr> </tbody> </table> <p>Since all outcomes in the sample space are equiprobable (each with probability 1/8), the probability of a particular value of the random variable \(X\) depends on how many outcomes produce that value. Hence:</p> \[P(X = 0) = \frac{1}{8}, \quad P(X = 1) = \frac{3}{8}, \quad P(X = 2) = \frac{3}{8}, \quad P(X = 3) = \frac{1}{8}\] <p><strong>See the table below along with the histogram plot in Fig. 2 below:</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/random_variable_and_distribution/output_as_histogram-480.webp 480w,/assets/img/random_variable_and_distribution/output_as_histogram-800.webp 800w,/assets/img/random_variable_and_distribution/output_as_histogram-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/random_variable_and_distribution/output_as_histogram.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 2</figcaption> </figure> <p><strong>We have seen the three examples of random variables above, all of which are <span style="color: green">Discrete random variables</span>. However, there are other types of random variables as well.</strong></p> <hr/> <h3 id="continuous-random-variables">Continuous Random Variables</h3> <p><strong>Amount of Rainfall in a Day:</strong></p> <p>The amount of rainfall in a day in a specific location is a continuous random variable. It can take any non-negative real value, from 0 mm on a dry day to several mm on a rainy day.</p> <p><strong>Speed of a Car on a Highway:</strong></p> <p>The speed of a car traveling on a highway is a continuous random variable. The speed can vary continuously over a range, such as between 0 km/h and the maximum speed limit, and can take any value within that range depending on traffic conditions and driver behaviour.</p> <p><strong>Blood Pressure Measurement:</strong></p> <p>The systolic blood pressure of a person is a continuous random variable. It can vary based on a person’s health, activity level, and other factors, and can take any value within a typical range, such as between 90 mmHg and 180 mmHg.</p> <p><strong>All of the above are examples of <span style="color: green">Continuous random variables.</span></strong></p> <hr/> <h3 id="discrete-vs-continuous">Discrete vs Continuous</h3> <p><strong>Let’s compare both side by side.</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/random_variable_and_distribution/contineous_and_discrete-480.webp 480w,/assets/img/random_variable_and_distribution/contineous_and_discrete-800.webp 800w,/assets/img/random_variable_and_distribution/contineous_and_discrete-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/random_variable_and_distribution/contineous_and_discrete.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 3</figcaption> </figure> <p><strong>So, the values that a discrete random variable can take can be put in a list as 1, 2, 3, 4, 5, etc. Whereas the values that a continuous random variable takes cannot be put in a list because it’s an entire interval.</strong></p> <p><strong>What is the difference between these random variables and the ones seen in algebra and calculus?</strong></p> <p>The variables in algebra and calculus are deterministic, whereas the ones here are random. What is deterministic? For example, in algebra if \(x = 2\), or in the case of function \(f(x) = x^2\) the input always takes the same value. Once it’s defined, it is fixed forever. However, a random variable is not like that. It can assume many values and is associated with an uncertain outcome. So deterministic variables are associated with a fixed outcome and random variables with an uncertain outcome, which is the primary distinction.</p> <hr/> <h3 id="terminologies">Terminologies</h3> <p>Now there are a few associated <strong>terminologies</strong> that we have to understand like <strong>Probability Mass Function (PMF), Probability Function (PF), Probability density function (PDF).</strong></p> <ol> <li> <p><strong>Probability Mass Function (PMF)</strong></p> <p><strong>Definition:</strong> The probability mass function (PMF) applies to discrete random variables. It provides the probability that a discrete random variable is exactly equal to some value.</p> <p><strong>Mathematical Representation:</strong> \(P(X = x_i) = p(x_i)\), where \(p(x_i)\) is the probability that the random variable \(X\) takes the value \(x_i\).</p> <p>Ex. For a fair six-sided die: \(P(X = x) = 1/6\) for \(x \in \{1,2,3,4,5,6\}\).</p> </li> <li> <p><strong>Probability Function</strong></p> <p><strong>Definition:</strong> The term “probability function” is sometimes used interchangeably with “probability mass function” or “probability density function” (for continuous random variables), depending on the context. Generally, it refers to any function that gives the probability of occurrence of different possible outcomes of an experiment.</p> <p><strong>Types:</strong></p> <ul> <li><strong>For Discrete Variables:</strong> The probability function is the PMF.</li> <li><strong>For Continuous Variables:</strong> The probability function is the Probability Density Function (PDF).</li> </ul> </li> </ol> <p>Details of probability density function will be covered in other part.</p>]]></content><author><name></name></author><category term="Math"/><category term="Math"/><category term="probability"/><category term="statistics"/><summary type="html"><![CDATA[Understand random variables with intuitive examples. Learn discrete vs continuous random variables, PMF, PDF, and probability distributions for machine learning.]]></summary></entry><entry><title type="html">Understanding Derivatives</title><link href="https://rami-rk.github.io/blog/2025/understanding-derivatives/" rel="alternate" type="text/html" title="Understanding Derivatives"/><published>2025-04-15T00:00:00+00:00</published><updated>2025-04-15T00:00:00+00:00</updated><id>https://rami-rk.github.io/blog/2025/understanding-derivatives</id><content type="html" xml:base="https://rami-rk.github.io/blog/2025/understanding-derivatives/"><![CDATA[<p>Let’s understand the intuition and physical meaning of the derivative with a simple example and after that, we will see the definition &amp; derivation of the derivative from the first principle.</p> <p>Let’s take a function \(y = x^2 + 2\), which is an equation of a parabola and plotted in Figure 1. Now I want to plot a tangent on this curve at the point (2, 6) and for this, I need the <strong>slope of the tangent line</strong> at that point. Please note that the slope is a measure of the steepness of the line.</p> <div style="text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/derivatives/parabola-480.webp 480w,/assets/img/derivatives/parabola-800.webp 800w,/assets/img/derivatives/parabola-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/derivatives/parabola.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 60%; " data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 1: Parabola</figcaption> </figure> </div> <hr/> <h3 id="calculating-slope-using-derivative">Calculating Slope using Derivative</h3> <p><strong>How to calculate slope? Derivative comes to the rescue.</strong></p> <p>We are going to calculate the derivative of the function, which gives the slope of the tangent at any particular point.</p> <p>Again, another question, how to calculate derivative?</p> <p>For the time being, we are going to use a few formulas listed below, and how we derived these formulas will be discussed later.</p> <p>Symbolically, Derivative of any function ‘y’ wrt ‘x’ is written as: \(\frac{dy}{dx}\)</p> <ol> <li> <p><strong>Derivative of any constant</strong>, say \(y = k\), where k can assume any real value:</p> \[\frac{dy}{dx} = \frac{dk}{dx} = 0\] <p>i.e. derivative of any constant is always zero.</p> </li> <li> <p><strong>Derivative of \(y = x\)</strong>, i.e. derivative of x with x itself:</p> \[\frac{dy}{dx} = \frac{dx}{dx} = 1\] <p>i.e. derivative of x with x itself is always 1.</p> </li> <li> <p><strong>Derivative of \(y = kx\)</strong>, where k is a constant:</p> \[\frac{dy}{dx} = \frac{d(kx)}{dx} = k \times 1 = k\] <p>Here constant ‘k’ came out of the derivative as it is and derivative of x wrt x is 1.</p> </li> <li> <p><strong>Derivative of \(y = x^n\)</strong>, where n is a constant exponent of x:</p> \[\frac{dy}{dx} = \frac{dx^n}{dx} = n \times x^{n-1} = nx^{n-1}\] <p>Here the exponent becomes coefficient and degrees of x decreased by 1.</p> </li> </ol> <p>Now, let’s calculate the derivative of function: \(y = x^2 + 2\) using above formulas, which gives slope of the tangent at any point.</p> \[\frac{dy}{dx} = \frac{d(x^2 + 2)}{dx} = \frac{d(x^2)}{dx} + \frac{d(2)}{dx} = 2x + 0 = 2x\] <p>Note that slope of the tangent at any point of function y is denoted by: \(\left(\frac{dy}{dx}\right)_{(x,y)}\)</p> <p>And as per our above calculation:</p> \[\left(\frac{dy}{dx}\right)_{(x,y)} = \frac{dy}{dx} = 2x\] <p>Finally, we can substitute the point \((x, y) = (2, 6)\) in above equation to get slope of the tangent at that point which results in:</p> \[\left(\frac{dy}{dx}\right)_{(2,6)} = 2 \times 2 = 4\] <p>So, we got the slope (\(m = 4\)) of the tangent line that passes through the point \((x_0 = 2, y_0 = 6)\) on the curve \(y = x^2 + 2\).</p> <p>To plot the tangent line, we can use the point-slope form of the straight-line equation, demonstrated below.</p> <div style="text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/derivatives/slope_point_eqn-480.webp 480w,/assets/img/derivatives/slope_point_eqn-800.webp 800w,/assets/img/derivatives/slope_point_eqn-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/derivatives/slope_point_eqn.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 70%; " data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> \[4 = \frac{y - 6}{x - 2}\] \[4x - 8 = y - 6\] <p>\(y = 4x - 2\) &lt;—- <strong>Equation of Tangent</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/derivatives/tangent_on_curve-480.webp 480w,/assets/img/derivatives/tangent_on_curve-800.webp 800w,/assets/img/derivatives/tangent_on_curve-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/derivatives/tangent_on_curve.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 80%; " data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 2: Tangent plot along with the Parabola</figcaption> </figure> <hr/> <h3 id="notation">Notation</h3> <p>You can find various notation to represent derivative or differentiation operation, all having the same meaning. Below are the notations for derivative of a function \(f(x)\) wrt <strong>x</strong>:</p> <table> <thead> <tr> <th style="text-align: center">Notation</th> <th style="text-align: left">Description</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">\(f'(x)\)</td> <td style="text-align: left">Prime notation (Lagrange)</td> </tr> <tr> <td style="text-align: center">\(\frac{df}{dx}\)</td> <td style="text-align: left">Leibniz notation</td> </tr> <tr> <td style="text-align: center">\(Df(x)\)</td> <td style="text-align: left">Euler notation</td> </tr> <tr> <td style="text-align: center">\(\frac{d}{dx} f(x)\)</td> <td style="text-align: left">Operator notation</td> </tr> </tbody> </table> <hr/> <h3 id="intuitive-interpretation">Intuitive Interpretation</h3> <p>We have understood the derivative through intuitive interpretation:</p> <ol> <li> <p><strong>Slope of the Tangent Line:</strong> The derivative at a point represents the slope of the tangent line to the graph of the function at that point.</p> </li> <li> <p><strong>Rate of Change:</strong> The derivative at a point gives the rate at which the function value is changing at that point. For example, if \(f(x)\) represents the position of an object over time, \(f'(x)\) represents the object’s velocity at time x.</p> </li> </ol> <hr/> <h3 id="formal-definition">Formal Definition</h3> <p>Given a function \(f(x)\), the derivative of ‘f’ at a point x is defined as the limit of the average rate of change of the function over an interval as the interval becomes infinitesimally small. Mathematically, this is expressed as:</p> \[f' = \frac{d}{dx} f(x) = \lim_{\Delta x \to 0} \frac{f(x + \Delta x) - f(x)}{\Delta x} \quad \text{--- (A)}\] <p>Here:</p> <ul> <li>\(f'\) is the derivative of \(f(x)\) at any point x; x can assume any value lying on the function.</li> <li>\(\Delta x\) is a small increment in x.</li> <li>\(\frac{f(x + \Delta x) - f(x)}{\Delta x} = \frac{\Delta y}{\Delta x}\) represents the average rate of change of the function over the interval from x to \((x + \Delta x)\).</li> </ul> <p>The meaning of eqn. (A) is best understood observing the figure below. The secant PQ represents the mean rate of change \(\frac{\Delta y}{\Delta x}\) of the function in the interval between x and \(x + \Delta x\).</p> <div style="text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/derivatives/defn_derivative_1-480.webp 480w,/assets/img/derivatives/defn_derivative_1-800.webp 800w,/assets/img/derivatives/defn_derivative_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/derivatives/defn_derivative_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 60%; " data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 3: Secant line PQ</figcaption> </figure> </div> <p>If we want the rate of change, say, at P we have to move point Q (and the secant with it) to meet point P, as shown in figure below.</p> <div style="text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/derivatives/defn_derivative_2-480.webp 480w,/assets/img/derivatives/defn_derivative_2-800.webp 800w,/assets/img/derivatives/defn_derivative_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/derivatives/defn_derivative_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 60%; " data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 4: Moving Q towards P</figcaption> </figure> </div> <p>In doing so, we must reduce \(\Delta x\). If \(\Delta x \to 0\), we will get the tangent at point P, shown by the red line, whose inclination/slope will give the slope at P and thus the derivative at P.</p> <hr/> <h3 id="example-from-first-principle">Example from First Principle</h3> <p><strong>Example of finding derivative from the first principle of the function \(f(x) = 3x^2 + 2x - 1\)</strong></p> \[\frac{d}{dx} f(x) = \lim_{\Delta x \to 0} \frac{f(x + \Delta x) - f(x)}{\Delta x}\] \[f'(x) = \lim_{\Delta x \to 0} \frac{3(x + \Delta x)^2 + 2(x + \Delta x) - 1 - (3x^2 + 2x - 1)}{\Delta x}\] \[f'(x) = \lim_{\Delta x \to 0} \frac{3x^2 + 6x\Delta x + 3\Delta x^2 + 2x + 2\Delta x - 1 - 3x^2 - 2x + 1}{\Delta x}\] \[f'(x) = \lim_{\Delta x \to 0} \frac{6x\Delta x + 3\Delta x^2 + 2\Delta x}{\Delta x}\] \[f'(x) = \lim_{\Delta x \to 0} (6x + 3\Delta x + 2) = 6x + 2\] <p>We have performed a simple derivative using the first principle for conceptual understanding. All the formulas we have used above, as well as the formulas for the derivatives of polynomial functions, trigonometric functions, exponential functions, and logarithmic functions, can be obtained using the first principle as demonstrated. These details are not necessary here, and one can refer to any intermediate calculus text for further study. Once we have the intuition, we can directly use the formulas for differential calculations of any function as needed.</p> <hr/> <h3 id="gradient">Gradient</h3> <p>You might have often heard the term <strong>‘Gradient’</strong> which is equivalent to slope in two-dimensional setting and can be used interchangeably. In the context of multivariable calculus, the gradient is a vector that represents the direction and rate of the steepest ascent of a scalar field. For a scalar function \(f(x, y, z)\) in three-dimensional (3D) space, the gradient is a vector given by:</p> \[\nabla f = \left( \frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}, \frac{\partial f}{\partial z} \right)\] <p>Concept of partial derivative will be covered later.</p> <hr/> <h3 id="derivatives-in-aiml">Derivatives in AIML</h3> <p><strong>How the concept of derivatives fits inside learning of AIML</strong></p> <p>In almost all optimization algorithms, the concept of the derivative is used, and ML models utilize these algorithms to find the best parameters at the end of the training process. Gradient descent is one such optimization algorithm used in many ML techniques, and the widespread success of deep learning is largely due to this.</p> <p>In gradient descent, the algorithm iteratively adjusts model parameters by computing the gradient of the loss function and moving a small step in the opposite direction. This process continues until the loss function is minimized, achieving optimal parameters.</p> <p><strong>Example:</strong> Let’s see one simple yet powerful verbal problem which convinces you to fully appreciate the concept of derivatives.</p> <p><strong>Problem Statement:</strong> A farmer wants to build a rectangular enclosure for her sheep. She has 100 meters of fencing material to use. What dimensions should she choose for the enclosure to maximize the area?</p> <p><strong>Solution:</strong> Let’s denote the length of the enclosure as \(l\) meters and the width as \(w\) meters. The perimeter of the rectangular enclosure is given by:</p> \[P = 2l + 2w\] <p>Since the farmer has 100 meters of fencing material, we have:</p> <p><strong>For Perimeter:</strong></p> \[2l + 2w = 100 \quad \text{---[I]}\] <p><strong>For area:</strong></p> <p>Say it is denoted by A, then \(A = l \times w\)</p> <p>Substituting for w in equation [I]:</p> \[A = l \times (50 - l) = 50l - l^2 \quad \text{---[II]}\] <p>As per the question we have to optimize the area. To find the maximum area, we need to find the critical points by taking the derivative of A with respect to l and setting it to zero:</p> \[\frac{dA}{dl} = 50 - 2l\] <p>Setting \(50 - 2l = 0\) gives \(l = 25\)</p> <p>Corresponding width is calculated using perimeter constraint, eqn. (I), which gives: \(w = 25\)</p> <p>The dimensions that maximize the area are \(l = 25\) meters and \(w = 25\) meters. The maximum area is:</p> \[A = l \cdot w = 25 \cdot 25 = 625 \text{ square meters}\] <p>Therefore, the farmer should choose dimensions of 25 meters by 25 meters to maximize the area of the enclosure.</p> <p>This is a simple example of optimization, where we have figured out best value of length and width which will maximize the area. This example is just to develop the intuition and show the application of derivative. More on optimization will be covered in subsequent articles.</p>]]></content><author><name></name></author><category term="Math"/><category term="Math"/><category term="calculus"/><category term="optimization"/><summary type="html"><![CDATA[Learn derivatives intuitively with visual examples. Understand slope, rate of change, gradient, and why derivatives matter for gradient descent in ML.]]></summary></entry><entry><title type="html">Self Attention</title><link href="https://rami-rk.github.io/blog/2025/self-attention/" rel="alternate" type="text/html" title="Self Attention"/><published>2025-03-26T00:00:00+00:00</published><updated>2025-03-26T00:00:00+00:00</updated><id>https://rami-rk.github.io/blog/2025/self-attention</id><content type="html" xml:base="https://rami-rk.github.io/blog/2025/self-attention/"><![CDATA[<h3 id="overview">Overview</h3> <p>Most of the popular language models are Transformer-based architectures that use an important technique called ‘self-attention’. The ‘self-attention’ is a little different from the attention mechanism used in the RNN-based encoder-decoder model. Let’s first try to understand the intuition before delving into mathematical details and equations.</p> <p>The primary function of self-attention is to generate context-aware vectors from the input sequence itself rather than considering both input and output as in the RNN-based encoder-decoder architecture. See the example below shown in Fig. 1 (Ref. Deep Learning with Python by François Chollet).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/self-attention/attention_matrix-480.webp 480w,/assets/img/self-attention/attention_matrix-800.webp 800w,/assets/img/self-attention/attention_matrix-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/self-attention/attention_matrix.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 1</figcaption> </figure> <p>In this example, there are 7 sequences in the sentence ‘the train left the station on time’, and we can see a 7x7 attention score matrix. For the time being, let’s say we somehow obtained these attention score values.</p> <p>According to the self-attention scores depicted in the picture, the word ‘train’ pays more attention to the word ‘station’ rather than other words in consideration, such as ‘on’ or ‘the’. Alternatively, we can say the word ‘station’ pays more attention to the word ‘train’ rather than other words in consideration, such as ‘on’ or ‘the’.</p> <p>The attention scores of each word in a sequence with all other words can be calculated, and the same is shown in the figure as a 7x7 score matrix. The self-attention model allows inputs to interact with each other (i.e., calculate the attention of all other inputs with one input).</p> <p>Attention scores help in understanding the contextual meaning of a word in a given sentence. For example, here the word ‘station’ has been used in the context of a train station, not in other contexts like a gas station or a bus station, etc.</p> <p>The attention score is computed through cosine similarity, i.e., the dot product of two-word vectors, which assesses the strength of their relationship or the degree of similarity between the compared words. There are many other mathematical aspects to consider, which will be discussed subsequently.</p> <p>These attention scores are utilized as weights for calculating the weighted sum of all the words in the sentence. For example, when representing the word ‘station’, the words closely related to ‘station’ will contribute more to the sum (including the word ‘station’ itself), while irrelevant words will contribute almost nothing. The resulting vector serves as a new representation of the word ‘station’, incorporating the surrounding context. Specifically, it includes part of the ‘train’ vector, thereby clarifying that it is, indeed, a ‘train station’.</p> <p>Let’s understand the above paragraph by writing a mathematical equation as given below, where \(A(i), \alpha(i,j)\) are weighted sum (termed as attention vector) and attention scores (weights) respectively, and individual word vector is represented by \(x_j\) . \(T_x\) represents the total number of terms in the sequence.</p> <p><strong>Note</strong> : The attention vector and weights can be written as:</p> \[A(i) = \sum_{j=1}^{T_x} \alpha(i,j) x_j\] <p>Unfolding the equation for the word “train”:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/self-attention/unfolding_attention_vector-480.webp 480w,/assets/img/self-attention/unfolding_attention_vector-800.webp 800w,/assets/img/self-attention/unfolding_attention_vector-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/self-attention/unfolding_attention_vector.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 1 (unfolding)</figcaption> </figure> <p>This process is iterated for every word in the sentence, yielding a new sequence of vectors that encode the sentence.</p> <hr/> <h3 id="calculation-of-attention-weights">Calculation of Attention Weights</h3> <p>Before delving into the mathematical details of the self-attention mechanism, let’s revisit the equations and terms derived from the attention mechanism in RNN-based encoder-decoder architectures (as explained in my initial article: <a href="/blog/2024/birth-of-attention-mechanism/">Birth of Attention Mechanism</a> ). Although it’s not necessary, to begin with the RNN-based attention mechanism to grasp self-attention, comparing term by term would facilitate comprehension and help us understand how identical concepts are used in both.</p> <p>A conceptual diagram is shown in Fig. 2 below to depict the attention mechanism in the RNN-based encoder-decoder model, where symbols have their usual meaning.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/self-attention/context_vector-480.webp 480w,/assets/img/self-attention/context_vector-800.webp 800w,/assets/img/self-attention/context_vector-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/self-attention/context_vector.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 65%; " data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 2</figcaption> </figure> <p><strong>Recall: in the case of Attention in RNN based encoder-decoder model:</strong></p> <p><strong>Context vector:</strong> \(C(i) = \sum_{j=1}^{T_x} \alpha_{i,j} h(j)\)</p> <p><strong>Score:</strong> \(e_{i,j} = f(S_{i-1}, h_j)\)</p> <p><strong>Attention score</strong> (SoftMax of score): \(\alpha_{i,j} = \frac{\exp(e_{i,j})}{\sum_{k=1}^{T} \exp(e_{i,k})}\)</p> <h3 id="self-attention">Self-Attention</h3> <p>In Self-Attention, we get rid of RNN units and calculate the context-aware vectors from the input sequence itself. A conceptual diagram is shown in Fig. 3 below to depict the Self-attention. Suppose we have a sequence of feature vectors (embeddings) \(X_1, X_2, \dots, X_T\)., in this case, T equals 4. We can use the concept of cosine similarity, which measures sameness or relatedness, to calculate scores and finally obtain the SoftMax, which can be used in the context vector as given below.</p> <p>In the case of Self-attention, the score is cosine similarity, i.e. dot product of the input sequence with itself. Thus,</p> <p><strong>Score:</strong> \(X_i^T X_j\) [Dot product of input vector with itself.]</p> <p><strong>Attention score:</strong> \(\alpha_{i,j} = \text{SoftMax}(\text{score} / \text{const.})\) [Dividing by a constant to control the variance.]</p> <p><strong>Self-Attention vector:</strong> \(A(i) = \sum_{j=1}^{T_x} \alpha_{i,j} x(j)\)</p> <p>Here \(h(j)\) is replaced by \(x_j\) and notation is changed from <strong>C</strong> to <strong>A</strong> as we call it <strong>“Attention Vector”</strong> insted of “context vector.”</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/self-attention/attention_vector-480.webp 480w,/assets/img/self-attention/attention_vector-800.webp 800w,/assets/img/self-attention/attention_vector-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/self-attention/attention_vector.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 65%; " data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 3</figcaption> </figure> <p><strong>Self-attention in vectorized form:</strong></p> \[\text{SoftMax}\left(\frac{X_i^T X_j}{\text{const.}}\right) \cdot x_j \quad ------ \text{(I)}\] <p>Table 1 shown below presents the side-by-side comparison of Self Attention Mechanism and RNN based Attention Mechanism.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/self-attention/table_comparison-480.webp 480w,/assets/img/self-attention/table_comparison-800.webp 800w,/assets/img/self-attention/table_comparison-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/self-attention/table_comparison.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Table 1: Comparison of self-attention and RNN-based attention</figcaption> </figure> <hr/> <h3 id="introducing-queries-keys-and-values">Introducing Queries, Keys, and Values</h3> <p>We computed the Self-Attention vector, as derived above, based on the inputs of the vectors themselves. This means that for fixed inputs, these attention weights would always be fixed. In other words, there are no learnable parameters. This is problematic and needs to be fixed by introducing some learnable parameters that will make the self-attention mechanism more flexible and tunable for various tasks. To fulfill this purpose, three trainable weight matrices are introduced and multiplied with input Xi separately, and three new terms Queries(Q), Keys(K), and Values(V) come into the picture as given by the equations below. Vectorized implementation &amp; Shape tracking are also shown in subsequent steps.</p> <p>Assume input ‘X’ is a sequence of ‘T’ time steps or in simple words a sentence with ‘T’ words and each word is represented by an Embedding vector of dimension ‘d_model’. Fig. 4 below shows multiple sentence samples (Xi’s) in rows S1, S2, S3, and so on, and words by column as ti=1, 2, 3, …, ti=T, which is say, up to 100.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/self-attention/input_data_sample-480.webp 480w,/assets/img/self-attention/input_data_sample-800.webp 800w,/assets/img/self-attention/input_data_sample-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/self-attention/input_data_sample.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 4</figcaption> </figure> <p>Notation and dimensions are followed as per the paper <a href="https://arxiv.org/abs/1706.03762" target="_blank">Attention Is All You Need</a>. Table 2 of shapes and Fig. 6 are taken from this paper.</p> <p><strong>Shape of</strong> \(X \rightarrow (T \times d_{model})\)</p> <p><strong>Queries:</strong> \(Q = XW^Q\), Where \(W^Q\) is weight matrix introduced to calculate Q from X.</p> <p><strong>Keys:</strong> \(K = XW^K\), Where \(W^K\) is weight matrix introduced to calculate K from X.</p> <p><strong>Values:</strong> \(V = XW^V\), Where \(W^V\) is weight matrix introduced to calculate V from X.</p> <p><strong>Shape tracking</strong> → (shape of \(X\) ) Matrix Mult. (shape of \(W^Q\) ) → <strong>Output shape</strong></p> <p>\(Q = XW^Q \rightarrow (T \times d_{model}) \cdot (d_{model} \times d_k) \rightarrow (T \times d_k)\) \(K = XW^K \rightarrow (T \times d_{model}) \cdot (d_{model} \times d_k) \rightarrow (T \times d_k)\) \(V = XW^V \rightarrow (T \times d_{model}) \cdot (d_{model} \times d_v) \rightarrow (T \times d_v)\)</p> <p>Finally, substituting Q, K, V, and \(\text{const.} = \sqrt{d_k}\) (\(d_k\) dimensionality of the key vector) in above self-attention eqn.(I) , we get equation for attention as given below:</p> \[\text{Attention}(Q, K, V) = \text{SoftMax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V \quad ------ \text{(II)}\] <p>Fig.5 below is a graphical representation of obtaining Q, K, and V from input X and feeding into equation (II) above</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/self-attention/scaled_dor_product_depiction1-480.webp 480w,/assets/img/self-attention/scaled_dor_product_depiction1-800.webp 800w,/assets/img/self-attention/scaled_dor_product_depiction1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/self-attention/scaled_dor_product_depiction1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 5</figcaption> </figure> <p>Above attention eqn. (II) is also termed as Scaled Dot Product Attention depicted in Fig. 6 given below.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/self-attention/scaled_dot_product_shapes1_m-480.webp 480w,/assets/img/self-attention/scaled_dot_product_shapes1_m-800.webp 800w,/assets/img/self-attention/scaled_dot_product_shapes1_m-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/self-attention/scaled_dot_product_shapes1_m.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 6</figcaption> </figure> <p>Shape of \(QK^T\): \((T \times d_k) \times (d_k \times T) \rightarrow (T \times T)\)</p> <p>And finally, the <strong>shape of final attention output</strong>: \((T \times T) \times (T \times d_v) → (T \times d_v)\)</p> <p>If we consider a batch of \(N\) samples at a time for processing, then above shape will be: \(N \times T \times d_v\). Please note that the shapes of \(W^Q, W^K, W^V\) are chosen in a way that matrix multiplication with input <strong>‘X’</strong> is possible. The values of \(d_k, d_v , d_model\) are hyperparameters and the table of shapes above shows the values used by the author in the paper.</p> <hr/> <h3 id="database-analogy-for-queries-keys-and-values">Database Analogy for Queries, Keys, and Values</h3> <p>In the context of databases, queries are used to interact with databases to retrieve or manipulate data, keys are used to uniquely identify records and establish relationships between tables, and values are the actual data stored in the fields of a database table. The same kind of analogy exists in Self-attention Q, K, and V as well, as shown in Fig. 7 below.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/self-attention/database_inspiration-480.webp 480w,/assets/img/self-attention/database_inspiration-800.webp 800w,/assets/img/self-attention/database_inspiration-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/self-attention/database_inspiration.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 7</figcaption> </figure> <p>In this example, the word ‘check’ acts as a query, and all the words in the sequence act as keys. Attention weights find the answer to the question: ‘Which key or what are the keys that match with the query?’ by assigning different weights to the keys. Here, the word ‘check’ pays more attention to the words ‘cashed’ and ‘bank’ in the sentence, as represented by the thickness of the joining lines.</p> <p>In self-attention, every word acts as a query once, while the entire words in the sequence act as keys, and attention weights are calculated to figure out which key matches the query.</p> <p>Finally, the words (represented as vectors) are treated as values, and attention weights are used to form a weighted sum of the values, resulting in an attention vector. The attention vector for the word ‘check’ will be the weighted sum of the value vectors.</p> <hr/> <h3 id="how-self-attention-helps-in-contextual-understanding">How Self-Attention Helps in Contextual Understanding</h3> <p>In the example we looked at, we were figuring out the meaning of a word. So, if we just see the word ‘check’ by itself, it could mean different things. But when we look at the other words in the sentence, like ‘cashed’ and ‘bank’, it helps us understand that in this context ‘check’ refers to a financial document, not something like checking your homework or check in chess.</p> <blockquote> <p><strong>Notes</strong></p> <ul> <li>Every word must have an attention weight with every other word, i.e., for \(T\) number of terms, there are \(T^2\) attention computations to calculate attention weights.</li> <li>Q, K, and V are calculated independently, resulting in parallelization, unlike in RNN where h(t-1) must be computed before h(t).</li> <li>Self-attention handles long sequences better than RNNs and avoids vanishing gradients.</li> </ul> </blockquote>]]></content><author><name></name></author><category term="AIML"/><category term="AIML"/><category term="deep-learning"/><category term="NLP"/><category term="transformers"/><summary type="html"><![CDATA[Master self-attention mechanism in transformers. Learn Queries, Keys, Values, scaled dot-product attention with intuitive examples and mathematical derivations.]]></summary></entry><entry><title type="html">Transformer Model Architecture</title><link href="https://rami-rk.github.io/blog/2024/transformer-architecture/" rel="alternate" type="text/html" title="Transformer Model Architecture"/><published>2024-03-27T00:00:00+00:00</published><updated>2024-03-27T00:00:00+00:00</updated><id>https://rami-rk.github.io/blog/2024/transformer-architecture</id><content type="html" xml:base="https://rami-rk.github.io/blog/2024/transformer-architecture/"><![CDATA[<h3 id="overview">Overview</h3> <p>In my attempt to demystify the Transformer Architecture, this is the third and last article. Here, I have explained all the components and their details in a very simplified way. Please read my previous two articles: <a href="/blog/2024/birth-of-attention-mechanism/">1. Birth of Attention Mechanism</a>, and <a href="/blog/2025/self-attention/">2. Self-Attention</a>, to fully appreciate and comprehend this article. These three articles will assist you in clarifying your understanding of the transformer, which serves as the foundation for recent advancements such as BERT, GPT, ChatGPT, and LLMs. Please note that these articles are not superficial overviews but rather technical in nature.</p> <p>The Fig.1 given below shows a Transformer Model Architecture as per the paper <a href="https://arxiv.org/abs/1706.03762" target="_blank">‘Attention Is All You Need’</a>. published in 2017 by A. Vaswani et al. It has two blocks — the left block is the Encoder and the right block is the Decoder. This transformer-based encoder-decoder architecture operates similarly to the RNN-based encoder-decoder that we saw by a translation example in the previous article — <a href="/blog/2024/birth-of-attention-mechanism/">Birth of Attention Mechanism</a> but without the presence of an RNN component. Despite this difference, the input-output feeding mechanism to the encoder-decoder remains consistent. Additionally, akin to the RNN model, the decoder functions as a causal model.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/tfr_full_archi-480.webp 480w,/assets/img/transformer/tfr_full_archi-800.webp 800w,/assets/img/transformer/tfr_full_archi-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/transformer/tfr_full_archi.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 1</figcaption> </figure> <p>The encoder sends Values(V) and Queries (Q) to the decoder, linking them together to create the entire Transformer system. This connection, known as cross-attention, enables the decoder to pay attention to the encoded information from the input sequence provided by the encoder. With this mechanism, the decoder can concentrate on different sections of the input sequence when producing the output sequence.</p> <p>It’s important to understand that the encoder and decoder components can function independently as standalone models for various tasks. For instance, BERT serves as an exclusive encoder architecture, while GPT operates solely as a decoder architecture. On the other hand, T5 incorporates both encoder and decoder components within its architecture.</p> <p>The Encoder and Decoder blocks share similar components, including Positional Encoding, Multi-Head Attention, Add &amp; Norm (comprising Skip connection and Layer normalization), and Feed Forward. However, one thing to notice is that as the decoder functions as a causal model, it employs causal attention or masked attention. This means that each token is restricted to attending only to its preceding tokens, not the subsequent ones.</p> <p><strong>Mastering the Encoder block facilitates comprehension of the Decoder block and the entire Transformer architecture. Let’s see Encoder in detail.</strong></p> <hr/> <h3 id="encoder">Encoder</h3> <p>The Encoder consists of a transformer block as shown in Fig. 2. A simplified and expanded diagram is also shown on the right side. Let’s understand the internal components of a Transformer block.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/enc_unfolded-480.webp 480w,/assets/img/transformer/enc_unfolded-800.webp 800w,/assets/img/transformer/enc_unfolded-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/transformer/enc_unfolded.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 2</figcaption> </figure> <p>The <strong>Transformer block</strong> can be sub-divided into <strong>two main sub-layers</strong>:</p> <ul> <li>The first sub-layer comprises a <strong>multi-head attention mechanism</strong> that receives the queries, keys, and values as inputs.</li> <li>A second sub-layer comprises a fully connected <strong>feed-forward network</strong>.</li> </ul> <p>Following each of these two sub-layers is layer normalization, into which the sub-layer input (through a residual/skip connection) and output are fed. Regularization is also introduced into the model by applying a dropout to the output of each sub-layer (before the layer normalization step) which is not shown in the figure.</p> <p>The feed-forward network allows the model to extract higher-level features from the input. This network usually comprises two linear layers with a ReLU activation function in between. The feed-forward network allows the model to extract deeper meaning from the input data and more compactly and usefully represent the input. In the paper, an ANN with one hidden layer and a ReLu activation in the middle with no activation function at the output layer has been implemented.</p> <p>The encoder is formed by repeated transformer block joined one after another multiple times which is shown as multiplied by N in Fig.2 above. The transformer encoder is a crucial part of the transformer encoder-decoder architecture, which is widely used for natural language processing tasks.</p> <p><strong>BERT</strong> which stands for Bi-directional encoder representation is an encoder-only architecture that consists of 6 to 12 such transformer blocks and a prediction head as the last layer, shown in Fig.3 below.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/tfr_blocks_assembed-480.webp 480w,/assets/img/transformer/tfr_blocks_assembed-800.webp 800w,/assets/img/transformer/tfr_blocks_assembed-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/transformer/tfr_blocks_assembed.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 3</figcaption> </figure> <hr/> <h3 id="multi-head-attention">Multi-Head Attention</h3> <p>Please go through the previous article on the <a href="/blog/2025/self-attention/">Self-Attention</a> mechanism, which is implemented in Multi-Head Attention. In Multi-Head Attention, the Attention module repeats its computations multiple times in parallel. Each of these is called an Attention Head. The Attention module splits its Query, Key, and Value parameters N-ways and passes each split independently through a separate Head. All of these similar Attention calculations are then combined to produce a final Attention score. This is called multi-head attention and gives the Transformer greater power to encode multiple relationships and nuances for each word. A schematic representation of MHA is shown in Fig.4 below.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/mha-480.webp 480w,/assets/img/transformer/mha-800.webp 800w,/assets/img/transformer/mha-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/transformer/mha.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 4</figcaption> </figure> <p>Shape tracking and related calculations are shown with the help of Fig. 5.:</p> <ul> <li>Input shape to each attention layer : \((T \times d_{model})\)</li> <li>Output shape after each attention layer : \((T \times d_V)\)</li> <li>After concatenation, the shape of Output (concatenate them along the feature dimension) : \((T \times h d_V)\)</li> </ul> <p>Imagine that the output of each self-attention block lined up side by side after concatenation.</p> <p><strong>Final projection:</strong> \(\text{Output} = \text{Concat}(A_1, A_2, \dots, A_h) W^O\)</p> <ul> <li>Shape of : \(\text{Concat}(A_1, \dots, A_h) \rightarrow (T \times h d_V)\)</li> <li>Shape of : \(W^O \rightarrow (h d_V \times d_{model})\)</li> <li>Shape of final Output : \(\text{Concat}(A_1, A_2, \dots, A_h) W^O \rightarrow (T \times h d_V) \times (h d_V \times d_{model})\)</li> </ul> <p>\(\rightarrow (T \times d_{model})\) &gt; <strong>Back to the initial input shape.</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/mh_concat-480.webp 480w,/assets/img/transformer/mh_concat-800.webp 800w,/assets/img/transformer/mh_concat-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/transformer/mh_concat.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 5</figcaption> </figure> <p>We can notice in the above figure that each attention calculation is functioning parallelly and has no dependency on the other.</p> <hr/> <h3 id="positional-encoding">Positional Encoding</h3> <p>Passing embeddings directly into the transformer block results in missing information about the order of tokens as we get rid of RNN blocks and attention is permutation invariant i.e. order of the token does not matter to attention. Although transformers are a sequence model, it appears that this important detail has somehow been lost. Positional encoding is for rescue. Positional encoding adds positional information to the existing embeddings.</p> <p>A unique set of numbers is added at each position of the existing embeddings, such that this new set of numbers can uniquely identify which position they are located at. The following two ways are there to add positional encoding:</p> <ol> <li> <p>Positional Encoding by Sub-Classing the Embedding Layer (Trainable)</p> </li> <li> <p>Positional Encoding scheme as per the paper (non-trainable)</p> </li> </ol> <p>In the scheme suggested in the paper, the encoding is created by using a set of sins and cosines at different frequencies. The paper uses the following formula for calculating the positional encoding.</p> \[PE_{(pos,\,2i)}=\sin\!\left(\frac{pos}{10000^{2i/d_{model}}}\right)\] \[PE_{(pos,\,2i+1)}=\cos\!\left(\frac{pos}{10000^{2i/d_{model}}}\right)\] <hr/> <h3 id="causal-or-masked-attention-in-decoder-block">Causal or Masked Attention in Decoder Block</h3> <p>All the above-mentioned components are found in both the encoder and decoder blocks. However, the decoder block utilizes a Causal Attention/Masked Attention Mechanism, differing slightly from the Attention mechanism in the encoder block. Let’s explore further.</p> <p>Decoder is a causal model i.e. it acts like a text generation tool, similar to how forecasts are made in time series analysis, where it predicts the next element based on previous ones. To accomplish this, when generating output at any position in the sequence, the model should only be permitted to consider the preceding tokens, excluding any tokens that followed.</p> <p>Let’s see mathematically how to achieve this through an attention score matrix. Fig.6(a) below shows the attention score matrix for just 5 tokens for the sake of demonstration. It will have a shape of \(T \times T\) for \(T\) sequences. Here, the <strong>attention score</strong> : \(\alpha (i, j)\) represents how much \(i\) pays attention to \(j\).</p> <p>Is it possible to restrict attention weights to consider only past input tokens? Yes, through some manipulation, we can achieve this: \(\alpha (i, j) &gt; 0\) only when \(i \ge j\), resulting in the score matrix depicted in Fig.6(b).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/masking-480.webp 480w,/assets/img/transformer/masking-800.webp 800w,/assets/img/transformer/masking-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/transformer/masking.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 6</figcaption> </figure> <p>We have masked the upper triangular elements to set them to zero. Thus, rather than employing plain Multi-Head Attention, we utilize a masked version known as ‘Causal Self-Attention.</p> <p>Finally, the <strong>Decoder</strong> is depicted in Fig. 7 below, which is equivalent to the Encoder, but the transformer block implements Masked Multi-Head Attention. All <strong>GPTs(Generative Pre-trained Transformers)</strong> are versions of decoder-only architecture with many more components attached to them, trained on a humongous amount of data.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/decoder_with_masked_mha-480.webp 480w,/assets/img/transformer/decoder_with_masked_mha-800.webp 800w,/assets/img/transformer/decoder_with_masked_mha-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/transformer/decoder_with_masked_mha.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 7</figcaption> </figure> <p>We have seen both the Encoder and Decoder separately. Encoder-only architecture is BERT while Decoder-only Architecture is GPT. Similarly, the architecture that contains both Encoder and Decoder is called <strong>T5 i.e. `Text to Text Transfer Transformer’</strong>. Fig. 8 below shows how Encoder and Decoder are connected in full Encoder and Decoder architecture.</p> <div style="text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/enc_dec_connection-480.webp 480w,/assets/img/transformer/enc_dec_connection-800.webp 800w,/assets/img/transformer/enc_dec_connection-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/transformer/enc_dec_connection.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 50%; " data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 8</figcaption> </figure> </div> <hr/> <h3 id="why-does-multi-head-attention-work">Why Does Multi-Head Attention Work</h3> <p>It allows the layer to learn multiple features. Think of attention outputs as features that tell us something about the sentence. For example, see Fig. 9 below, when Suman pays attention to bank, the feature might be Where did Suman go? But we might also be interested in what Suman did. He cashed the check. We can only do this if we allow our layer to produce multiple features, each of which has the Suman token pay attention to different tokens in the sentence. There would be multiple and different kind of relationships and dependencies between different tokens in any sequence. Multi-head attention helps in learning those different relationships by learning corresponding multiple features.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transformer/intuition_mha-480.webp 480w,/assets/img/transformer/intuition_mha-800.webp 800w,/assets/img/transformer/intuition_mha-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/transformer/intuition_mha.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 9</figcaption> </figure>]]></content><author><name></name></author><category term="AIML"/><category term="AIML"/><category term="deep-learning"/><category term="NLP"/><category term="transformers"/><summary type="html"><![CDATA[Complete guide to Transformer architecture. Learn encoder, decoder, multi-head attention, positional encoding, and masked attention with detailed explanations.]]></summary></entry><entry><title type="html">Birth of Attention Mechanism</title><link href="https://rami-rk.github.io/blog/2024/birth-of-attention-mechanism/" rel="alternate" type="text/html" title="Birth of Attention Mechanism"/><published>2024-03-26T00:00:00+00:00</published><updated>2024-03-26T00:00:00+00:00</updated><id>https://rami-rk.github.io/blog/2024/birth-of-attention-mechanism</id><content type="html" xml:base="https://rami-rk.github.io/blog/2024/birth-of-attention-mechanism/"><![CDATA[<h3 id="overview">Overview</h3> <p>The onset of ChatGPT and many other open-source LLMs have revolutionized the world and the way work happens in all sorts of industries. But very few of us know that transformer-based models are the mother of all these magic tools- including BERT, GPT, and all the buzzwords around large language models.</p> <p>I have struggled to understand the underlying concept and the working principle of <strong>Transformer</strong>, and I believe many struggle in the same way.</p> <p>I am going to publish a series of articles on the Transformer to simplify the concept and break down things into small pieces. I believe this will help many to understand these complex concepts easily and help them to break into this domain.</p> <p>To understand the transformer architecture, first, we have to understand the attention mechanism- the main component of transformer architecture and the concept of self-attention, and finally, the full Transformer Architecture. Maintaining the conceptual logic above, the following are the three articles that I am going to publish. The first article in the series is <strong>“The Birth of Attention Mechanism”</strong>.The second article in the series is <strong>“Self-Attention”</strong>, and the third article is <strong>“Transformer Model Archetecture”</strong>.</p> <p>This is the first article and we are going to understand the attention mechanism in the context of language translation using seq2seq (encoder-decoder) architecture. In the example below, a Hindi sentence <strong>“मुझे खाना पसंद है”</strong> is fed into an encoder which has been translated(decoded) by decoder into English as <strong>“I love to eat”</strong>.</p> <p>The encoder is used to ingest the input. It simply computes all the \(h \text{ of T’s} (\text{ i.e. } h(1), h(2), …)\) at every time step of the input and gives one final hidden state vector, which we’ll call \(h(T)\), assuming that the input sequence has a length of \(T\).</p> <p>Here, \(h(T)\) is the output of the encoder, which is also the input to the decoder. We can think of \(h(T)\) as a vector, a compressed representation of the input sequence.</p> <blockquote> <p>\(h(T)\): A compressed representation of the input. → “Thought Vector”</p> </blockquote> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/birth-of-attention-mechanism/enc_dec_rnn-480.webp 480w,/assets/img/birth-of-attention-mechanism/enc_dec_rnn-800.webp 800w,/assets/img/birth-of-attention-mechanism/enc_dec_rnn-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/birth-of-attention-mechanism/enc_dec_rnn.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 1</figcaption> </figure> <hr/> <h3 id="lets-see-the-decoder-in-detail">Lets See the Decoder in Detail</h3> <p>Every RNN unit has two sources of input data, the first being the actual input and the second being the previous head state. The decoder is also an RNN but a different RNN from the encoder. In this diagram above, the inputs go along the bottom of the RNN unit while the previous head state comes through the left. For a normal RNN, the initial hidden state is usually just a vector of zeros, or maybe some trainable but fixed vector. In this case, for the decoder, the initial hidden state comes from the output of the encoder, which is \(h(T)\). Thus, the job of the decoder is to decompress the compressed representation of the input.</p> <hr/> <h3 id="what-goes-into-the-bottom-of-the-decoder-rnn-unit">What Goes Into the Bottom of the Decoder RNN Unit</h3> <p>At the beginning, there’s a special token called \(\text{&lt;SOS&gt;}\) which indicates the start of a sentence or sequence. The decoder RNN acts like a text generation tool, similar to how forecasts are made in time series analysis, where it predicts the next element based on previous ones, such a model is also known as an autoregressive model or causal model. So, starting with the \(\text{&lt;SOS&gt;}\) token and an initial hidden state, we predict the first word of the translated sequence. After getting this first word, we put it back into the decoder as input, using it to predict the second word in the sequence. We keep repeating this process until we’ve generated the entire translated sequence. To know when to stop, we watch for the special end token produced by our model, signaling that the translation is complete.</p> <p>Note that, we need a lot of data sets containing paired Hindi sentences and equivalent English sentences to train the model, and only after the model performs the translation as shown in the diagram above.</p> <hr/> <h3 id="problem-with-seq2seq">Problem With Seq2Seq</h3> <ol> <li>Every input sequence is converted using an encoder RNN into a single vector. Here’s the problem — What if the input sentence is very long? The final encoder state \(h(T)\) always has the same size and has to remember the whole input.</li> <li>Humans do not translate by remembering entire sentences at once; we focus on relevant parts as we go.</li> </ol> <hr/> <blockquote> <p><strong>To Solve This Problem: Attention is Originated for Seq2Seq</strong></p> </blockquote> <div style="text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/birth-of-attention-mechanism/attention_def-480.webp 480w,/assets/img/birth-of-attention-mechanism/attention_def-800.webp 800w,/assets/img/birth-of-attention-mechanism/attention_def-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/birth-of-attention-mechanism/attention_def.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 70%; " data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 2</figcaption> </figure> </div> <p>Let’s try to understand this attention concept with a simple example, where the input: <strong>“मेँ खेलने जा रहा हूँ “</strong> fed into an encoder and translated into “I am going to play” as an output from the decoder as shown in fig.3 below. In this figure ‘i/p’ at the bottom represents the input sequence and <strong>‘o/p’</strong> at the top represents the output sequence.</p> <p>Here, the list ‘t1’ contains the values for attention that output at time step 1 should pay attention to all input sequences. The list <strong>‘t2’</strong> contains the values for attention that output at time step 2 should pay attention to all input sequences and similarly other lists also represent attention values for input sequences for different output at different time steps.</p> <div style="text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/birth-of-attention-mechanism/attention_values-480.webp 480w,/assets/img/birth-of-attention-mechanism/attention_values-800.webp 800w,/assets/img/birth-of-attention-mechanism/attention_values-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/birth-of-attention-mechanism/attention_values.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 70%; " data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 3</figcaption> </figure> </div> <p>Let’s understand the attention values inside list <strong>‘t1’</strong>. When we’re aiming to generate the first word ‘I’ in our output, what we’re doing is figuring out the likelihood of each input word being the one we should focus on. At this stage, it’s fine if we don’t know the translations for <strong>‘खेलने’</strong> or <strong>‘जा रह’</strong> or <strong>‘हू’</strong>, as long as we know the translation for <strong>‘मेँ’</strong> because that’s the word we need to generate first. So, we can simplify things by saying that, at this point, we only need to pay attention to the first word in the input and can ignore everything else. This is the reason the attention value is 1 for the first input (मेँ) and 0 for all other input sequences.</p> <p>What about the second time step word <strong>“am”</strong> in output? We just need to focus on the last word हूँ in the input, and thus, the list ‘t2’ contains attention value 1 at last and 0 for all other input sequences.</p> <p>What about the third time step word <strong>“going”</strong> in output? Is it always going to be that we only need to focus on one word at a time? No. We will focus on two input sequences <strong>‘जा‘</strong> and <strong>‘रहा’</strong> and ignore everything else. This is the reason, the list t3 contains attention values 0.5 and 0.5 at respective time step positions of <strong>‘जा‘</strong> and <strong>‘रहा’</strong> and the remaining all are zero. Similarly, for the remaining time step.</p> <p>Is this the encoder-decoder architecture doing, that we saw previously in Fig.1? No! Every time step focuses on the encoding of the entire sentence (i.e. final h(T)) because that is the encoding that we are feeding to every time step. This is the problem we need to fix.</p> <p>We need to learn to pay attention to certain important parts of the sentence. Ideally, at each time step, we should feed only the relevant information i.e. encodings of relevant words to the decoder.</p> <hr/> <h3 id="improved-encoder-decoder-architecture-with-attention">Improved Encoder-Decoder Architecture With Attention</h3> <p>Let’s see how to improve the previous encoder-decoder architecture with the concept of Attention.</p> <p>In the given Fig.3 below, the decoder is drawn on top of the encoder and instead of feeding only the last hidden state of the encoder to the decoder all hidden states are fed into the decoder at each time step with some weightage noted by alpha. For the sake of simplicity, connection for only output at time state \(t=2 \text{ and } t=4\) have shown.</p> <p>A weighted combination of each output is taken from the encoder cells (i.e., vector of the same shape) and fed into each step cell of the decoder. Assume for the time being that, we got these alpha’s (i.e. attention weights for each hidden state from the encoder) from some magic, we will figure out how to calculate it.:</p> <blockquote> <p><strong>For output at the time state t=2 ie. ‘Love’:</strong> \(\alpha_{2,1} \text{ is the weight of attention that is should pay to input at time state 1}\) \(\alpha_{2,2} \text{ is the weight of attention that is should pay to input at time state 2}\) \(\alpha_{2,3} \text{ is the weight of attention that is should pay to input at time state 3}\) \(\alpha_{2,4} \text{ is the weight of attention that is should pay to input at time state 4}\)</p> </blockquote> <blockquote> <p><strong>Similarly, for output at time step t = 4 i.e. ‘eat’:</strong> \(\alpha_{4,1} \text{ is the weight of attention that is should pay to input at time state 1}\) , and similarly remainings: \(\alpha_{4,2}, \alpha_{4,3}, \text{ and }\alpha_{4,4}\)</p> </blockquote> <div style="text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/birth-of-attention-mechanism/rnn_with_attention-480.webp 480w,/assets/img/birth-of-attention-mechanism/rnn_with_attention-800.webp 800w,/assets/img/birth-of-attention-mechanism/rnn_with_attention-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/birth-of-attention-mechanism/rnn_with_attention.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 70%; " data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 4</figcaption> </figure> </div> <hr/> <h3 id="context-vector">Context Vector</h3> <p>Now, defining weighted combination as a context vector, say for output at time step 4:</p> \[C(4) = \alpha_{4,1} h(1) + \alpha_{4,2} h(2) + \alpha_{4,3} h(3) + \alpha_{4,4} h(4)\] <p>Denoting :</p> <ul> <li>Input time step by j = 1, 2 ,.., T</li> <li>Output time step by i = 1, 2 ,.., t</li> </ul> <p>We can generalize the above equation as a <strong>Context Vector</strong>:</p> \[C(i) = \sum_{j=1}^{T_x} \alpha_{i,j} h(j)\] <p>Here, the context vector is simply the weighted sum of h’s where the weights are the alphas. So, this is the basic idea behind the attention. We have weights from every input to every output, which tells us how much each output should pay attention to which input.</p> <hr/> <h3 id="calculating-attention-weights">Calculating Attention Weights</h3> <p>Before calculating alphas, let’s introduce a term called <strong>alignment score</strong> noted by $e_{i,j}$ and defined as: at <strong>i<sup>th</sup></strong> time step of decoder, how important is the <strong>j<sup>th</sup></strong> word in the input. This should depend on or should be a function of j<sup>th</sup> word and <strong>whatever has happened in the decoder</strong> so far. Note, S₁, S₂…, Sₜ are decoder side hidden states.</p> \[score: e_{i,j} = f(s_{i-1}, h_j)\] <p>We are interested in $e_{i,j}$ for all the input words. For <strong>j<sup>th</sup></strong> input word, we are interested in knowing how important it is for the <strong>i<sup>th</sup></strong> time-step output of the decoder. <strong>There are several ways this function can be defined and no need to know at this moment but the intuition that it depends on $s_{i-1}$, and $h_j$ is sufficient.</strong></p> <p>Now across all the input words, we want the sum of this score value to be one. We don’t want some arbitrary weights. It is just like probability distribution over what word is important by how much. We can achieve this by taking <strong>SoftMax of the score</strong>.</p> \[\alpha_{i,j} = \frac{exp(e_{i,j})}{\sum_{k=1}^{T} exp(e_{i,k})}\] <p><strong>Attention Weight, $\alpha_{i,j}$</strong> denotes the probability of focusing on the <strong>j<sup>th</sup></strong> word to produce <strong>i<sup>th</sup></strong> output word.</p>]]></content><author><name></name></author><category term="AIML"/><category term="AIML"/><category term="deep-learning"/><category term="NLP"/><category term="transformers"/><summary type="html"><![CDATA[Learn how attention mechanism works in seq2seq encoder-decoder models. Understand context vectors, attention weights, and why attention solves the long sequence problem.]]></summary></entry></feed>